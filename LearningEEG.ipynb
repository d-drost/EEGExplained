{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4261999d-cdc5-4d36-8e10-9ae84155f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4386/2342651398.py:8: RuntimeWarning: Estimated head radius (0.0 cm) is below the 3rd percentile for infant head size. Check if the montage_units argument is correct (the default is \"mm\", but your channel positions may be in different units).\n",
      "  raw = read_raw_eeglab(\"../sub400/400MNE.set\")\n",
      "/tmp/ipykernel_4386/2342651398.py:11: RuntimeWarning: The unit for channel(s) AUX1, AUX2, AUX3, AUX4, BIP2, BIP3, BIP4, M1, M2 has changed from V to NA.\n",
      "  raw.set_channel_types({\"BIP1\":\"eog\",\"BIP2\":\"misc\",\"BIP3\":\"misc\",\"BIP4\":\"misc\", \"AUX1\":\"misc\",\"AUX2\":\"misc\",\"AUX3\":\"misc\",\"AUX4\":\"misc\", \"M1\":\"misc\", \"M2\":\"misc\"})\n",
      "/tmp/ipykernel_4386/2342651398.py:12: RuntimeWarning: Not setting positions of 2 misc channels found in montage:\n",
      "['M1', 'M2']\n",
      "Consider setting the channel types to be of EEG/sEEG/ECoG/DBS/fNIRS using inst.set_channel_types before calling inst.set_montage, or omit these channels when creating your montage.\n",
      "  raw.set_montage(make_standard_montage(\"standard_1020\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "    const toggleVisibility = (className) => {\n",
       "\n",
       "  const elements = document.querySelectorAll(`.${className}`)\n",
       "\n",
       "  elements.forEach(element => {\n",
       "    if (element.classList.contains('repr-section-header')) {\n",
       "      // Don't collapse the section header row.\n",
       "       return\n",
       "    }\n",
       "    if (element.classList.contains('repr-element-collapsed')) {\n",
       "      // Force a reflow to ensure the display change takes effect before removing the class\n",
       "      element.classList.remove('repr-element-collapsed')\n",
       "      element.offsetHeight // This forces the browser to recalculate layout\n",
       "      element.classList.remove('repr-element-faded')\n",
       "    } else {\n",
       "      // Start transition to hide the element\n",
       "      element.classList.add('repr-element-faded')\n",
       "      element.addEventListener('transitionend', handler = (e) => {\n",
       "        if (e.propertyName === 'opacity' && getComputedStyle(element).opacity === '0.2') {\n",
       "          element.classList.add('repr-element-collapsed')\n",
       "          element.removeEventListener('transitionend', handler)\n",
       "        }\n",
       "      });\n",
       "    }\n",
       "  });\n",
       "\n",
       "  // Take care of button (adjust caret)\n",
       "  const button = document.querySelectorAll(`.repr-section-header.${className} > th.repr-section-toggle-col > button`)[0]\n",
       "  button.classList.toggle('collapsed')\n",
       "\n",
       "  // Take care of the tooltip of the section header row\n",
       "  const sectionHeaderRow = document.querySelectorAll(`tr.repr-section-header.${className}`)[0]\n",
       "  sectionHeaderRow.classList.toggle('collapsed')\n",
       "  sectionHeaderRow.title = sectionHeaderRow.title === 'Hide section' ? 'Show section' : 'Hide section'\n",
       "}\n",
       "</script>\n",
       "\n",
       "<style type=\"text/css\">\n",
       "    table.repr.table.table-hover.table-striped.table-sm.table-responsive.small {\n",
       "  /* Don't make rows wider than they need to be. */\n",
       "  display: inline;\n",
       "}\n",
       "\n",
       "table > tbody > tr.repr-element > td {\n",
       "  /* Apply a tighter layout to the table cells. */\n",
       "  padding-top: 0.1rem;\n",
       "  padding-bottom: 0.1rem;\n",
       "  padding-right: 1rem;\n",
       "}\n",
       "\n",
       "table > tbody > tr > td.repr-section-toggle-col {\n",
       "  /* Remove background and border of the first cell in every row\n",
       "     (this row is only used for the collapse / uncollapse caret)\n",
       "\n",
       "     TODO: Need to find a good solution for VS Code that works in both\n",
       "           light and dark mode. */\n",
       "  border-color: transparent;\n",
       "  --bs-table-accent-bg: transparent;\n",
       "}\n",
       "\n",
       "tr.repr-section-header {\n",
       "  /* Remove stripes from section header rows */\n",
       "  background-color: transparent;\n",
       "  border-color: transparent;\n",
       "  --bs-table-striped-bg: transparent;\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       "tr.repr-section-header > th {\n",
       "  text-align: left !important;\n",
       "  vertical-align: middle;\n",
       "}\n",
       "\n",
       ".repr-element, tr.repr-element > td {\n",
       "  opacity: 1;\n",
       "  text-align: left !important;\n",
       "}\n",
       "\n",
       ".repr-element-faded {\n",
       "  transition: 0.3s ease;\n",
       "  opacity: 0.2;\n",
       "}\n",
       "\n",
       ".repr-element-collapsed {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "/* Collapse / uncollapse button and the caret it contains. */\n",
       ".repr-section-toggle-col button {\n",
       "  cursor: pointer;\n",
       "  width: 1rem;\n",
       "  background-color: transparent;\n",
       "  border-color: transparent;\n",
       "}\n",
       "\n",
       "span.collapse-uncollapse-caret {\n",
       "  width: 1rem;\n",
       "  height: 1rem;\n",
       "  display: block;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: left;\n",
       "  background-size: contain;\n",
       "}\n",
       "\n",
       "/* The collapse / uncollapse carets were copied from the free Font Awesome collection and adjusted. */\n",
       "\n",
       "/* Default to black carets for light mode */\n",
       ".repr-section-toggle-col > button.collapsed > span.collapse-uncollapse-caret {\n",
       "  background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"black\" d=\"M246.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-9.2-9.2-22.9-11.9-34.9-6.9s-19.8 16.6-19.8 29.6l0 256c0 12.9 7.8 24.6 19.8 29.6s25.7 2.2 34.9-6.9l128-128z\"/></svg>');\n",
       "}\n",
       "\n",
       ".repr-section-toggle-col\n",
       "  > button:not(.collapsed)\n",
       "  > span.collapse-uncollapse-caret {\n",
       "  background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"black\" d=\"M137.4 374.6c12.5 12.5 32.8 12.5 45.3 0l128-128c9.2-9.2 11.9-22.9 6.9-34.9s-16.6-19.8-29.6-19.8L32 192c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9l128 128z\"/></svg>');\n",
       "}\n",
       "\n",
       "/* Use white carets for dark mode */\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .repr-section-toggle-col > button.collapsed > span.collapse-uncollapse-caret {\n",
       "    background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"white\" d=\"M246.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-9.2-9.2-22.9-11.9-34.9-6.9s-19.8 16.6-19.8 29.6l0 256c0 12.9 7.8 24.6 19.8 29.6s25.7 2.2 34.9-6.9l128-128z\"/></svg>');\n",
       "  }\n",
       "\n",
       "  .repr-section-toggle-col\n",
       "    > button:not(.collapsed)\n",
       "    > span.collapse-uncollapse-caret {\n",
       "    background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"white\" d=\"M137.4 374.6c12.5 12.5 32.8 12.5 45.3 0l128-128c9.2-9.2 11.9-22.9 6.9-34.9s-16.6-19.8-29.6-19.8L32 192c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9l128 128z\"/></svg>');\n",
       "  }\n",
       "}\n",
       "\n",
       ".channel-names-btn {\n",
       "  padding: 0;\n",
       "  border: none;\n",
       "  background: none;\n",
       "  text-decoration: underline;\n",
       "  text-decoration-style: dashed;\n",
       "  cursor: pointer;\n",
       "  color: #0d6efd;\n",
       "}\n",
       "\n",
       ".channel-names-btn:hover {\n",
       "  color: #0a58ca;\n",
       "}\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "<table class=\"repr table table-hover table-striped table-sm table-responsive small\">\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header general-bcca3697-627e-4a3e-9260-2c0efe6b3761\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('general-bcca3697-627e-4a3e-9260-2c0efe6b3761')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>General</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-bcca3697-627e-4a3e-9260-2c0efe6b3761 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Filename(s)</td>\n",
       "    <td>\n",
       "        \n",
       "        400MNE.fdt\n",
       "        \n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-bcca3697-627e-4a3e-9260-2c0efe6b3761 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>MNE object type</td>\n",
       "    <td>RawEEGLAB</td>\n",
       "</tr>\n",
       "<tr class=\"repr-element general-bcca3697-627e-4a3e-9260-2c0efe6b3761 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Measurement date</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-bcca3697-627e-4a3e-9260-2c0efe6b3761 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Participant</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-bcca3697-627e-4a3e-9260-2c0efe6b3761 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Experimenter</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header acquisition-199746da-4b1e-436e-a6b0-eeef73a3b5cf\" \n",
       "    title=\"Hide section\"  onclick=\"toggleVisibility('acquisition-199746da-4b1e-436e-a6b0-eeef73a3b5cf')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Acquisition</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element acquisition-199746da-4b1e-436e-a6b0-eeef73a3b5cf \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Duration</td>\n",
       "    <td>00:04:60 (HH:MM:SS)</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-199746da-4b1e-436e-a6b0-eeef73a3b5cf \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Sampling frequency</td>\n",
       "    <td>512.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-199746da-4b1e-436e-a6b0-eeef73a3b5cf \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Time points</td>\n",
       "    <td>153,600</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header channels-e3dfe0b8-232b-4406-a817-f160d4091965\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('channels-e3dfe0b8-232b-4406-a817-f160d4091965')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Channels</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-e3dfe0b8-232b-4406-a817-f160d4091965 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>EEG</td>\n",
       "    <td>\n",
       "        <button class=\"channel-names-btn\" onclick=\"alert('Good EEG:\\n\\nFp1, Fpz, Fp2, F7, F3, Fz, F4, F8, FC5, FC1, FC2, FC6, T7, C3, Cz, C4, T8, CP5, CP1, CP2, CP6, P7, P3, Pz, P4, P8, POz, O1, Oz, O2, AF7, AF3, AF4, AF8, F5, F1, F2, F6, FC3, FCz, FC4, C5, C1, C2, C6, CP3, CPz, CP4, P5, P1, P2, P6, PO5, PO3, PO4, PO6, FT7, FT8, TP7, TP8, PO7, PO8')\" title=\"(Click to open in popup)&#13;&#13;Fp1, Fpz, Fp2, F7, F3, Fz, F4, F8, FC5, FC1, FC2, FC6, T7, C3, Cz, C4, T8, CP5, CP1, CP2, CP6, P7, P3, Pz, P4, P8, POz, O1, Oz, O2, AF7, AF3, AF4, AF8, F5, F1, F2, F6, FC3, FCz, FC4, C5, C1, C2, C6, CP3, CPz, CP4, P5, P1, P2, P6, PO5, PO3, PO4, PO6, FT7, FT8, TP7, TP8, PO7, PO8\">\n",
       "            62\n",
       "        </button>\n",
       "\n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-e3dfe0b8-232b-4406-a817-f160d4091965 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>EOG</td>\n",
       "    <td>\n",
       "        <button class=\"channel-names-btn\" onclick=\"alert('Good EOG:\\n\\nBIP1')\" title=\"(Click to open in popup)&#13;&#13;BIP1\">\n",
       "            1\n",
       "        </button>\n",
       "\n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-e3dfe0b8-232b-4406-a817-f160d4091965 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>misc</td>\n",
       "    <td>\n",
       "        <button class=\"channel-names-btn\" onclick=\"alert('Good misc:\\n\\nM1, M2, BIP2, BIP3, BIP4, AUX1, AUX2, AUX3, AUX4')\" title=\"(Click to open in popup)&#13;&#13;M1, M2, BIP2, BIP3, BIP4, AUX1, AUX2, AUX3, AUX4\">\n",
       "            9\n",
       "        </button>\n",
       "\n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-e3dfe0b8-232b-4406-a817-f160d4091965 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Head & sensor digitization</td>\n",
       "    \n",
       "    <td>65 points</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header filters-fabf17f0-abfb-4be8-b1b6-c6212d1a6ca7\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('filters-fabf17f0-abfb-4be8-b1b6-c6212d1a6ca7')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Filters</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element filters-fabf17f0-abfb-4be8-b1b6-c6212d1a6ca7 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Highpass</td>\n",
       "    <td>0.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element filters-fabf17f0-abfb-4be8-b1b6-c6212d1a6ca7 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Lowpass</td>\n",
       "    <td>256.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEEGLAB | 400MNE.fdt, 72 x 153600 (300.0 s), ~84.5 MB, data loaded>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mne.io import read_raw_eeglab, read_raw\n",
    "from mne.channels import get_builtin_montages, make_standard_montage\n",
    "from mne import events_from_annotations, Epochs\n",
    "import PIL\n",
    "import mne\n",
    "raw = None\n",
    "mne.set_log_level(False)\n",
    "raw = read_raw_eeglab(\"../sub400/400MNE.set\")\n",
    "raw = raw.crop(tmin=60, tmax=60+5*60)\n",
    "raw = raw.copy().resample(512)\n",
    "raw.set_channel_types({\"BIP1\":\"eog\",\"BIP2\":\"misc\",\"BIP3\":\"misc\",\"BIP4\":\"misc\", \"AUX1\":\"misc\",\"AUX2\":\"misc\",\"AUX3\":\"misc\",\"AUX4\":\"misc\", \"M1\":\"misc\", \"M2\":\"misc\"})\n",
    "raw.set_montage(make_standard_montage(\"standard_1020\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fffd4e0-6c24-4774-8336-9965f6705798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Textarea, Text, Label, Button, VBox, HBox, HTML, Image, Tab, Output, Layout, FloatSlider, Dropdown, FloatRangeSlider, Checkbox, RadioButtons\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import markdown\n",
    "\n",
    "width,height = 600, 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48cda6bc-e843-43a7-b9f4-b72483168b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_to_reveal(text, answer, with_input_field=False):\n",
    "    test_reveal_button = Button(description=\"Check answer\")\n",
    "    \n",
    "    test_answer = HTML(answer)\n",
    "    test_answer.layout.visibility=\"hidden\"\n",
    "    def show_answer(b):\n",
    "        test_answer.layout.visibility=\"visible\"\n",
    "    test_reveal_button.on_click(show_answer)\n",
    "    layout = Layout(width=\"50%\", border=\"solid purple\")\n",
    "    if with_input_field:\n",
    "        elements = VBox([HTML(text), HBox([Text(), test_reveal_button]), test_answer], layout=layout)\n",
    "    else:\n",
    "        elements = VBox([HTML(text), HBox([test_reveal_button]), test_answer], layout=layout)\n",
    "    return elements\n",
    "    \n",
    "def get_chapter(panels, panel_titles):\n",
    "    tab = Tab()\n",
    "    tab.children = panels\n",
    "    tab.titles = panel_titles\n",
    "    chapter = tab\n",
    "    return chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee50d5be-c9ea-4e64-b76f-0d3b9f16eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def code_block(code, links=None):\n",
    "    code_block_html = HTML(f\"\"\"\n",
    "    <pre><code>\n",
    "    {code}\n",
    "    </code></pre>\"\"\")\n",
    "    if links is not None:\n",
    "        sources = [f\"\"\"<a href=\"{link}\" target=\"_blank\"><u>{name}</u></a>\"\"\" for name, link in links.items()]\n",
    "        sources = \"\\n\".join(sources)\n",
    "        sources = \"References: \" + sources\n",
    "        sources = HTML(sources)\n",
    "        \n",
    "        sources.style.background = \"#d3f705\"\n",
    "        ret = VBox([code_block_html, sources])\n",
    "    else:\n",
    "        ret = code_block_html\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864bb715-9273-4fd1-8a1b-04912de72de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(d):\n",
    "    ch_names = d.ch_names\n",
    "    assert type(ch_names) == list and len(ch_names) > 0\n",
    "    assert raw.n_times > 60\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23636c8f-48db-40bc-a052-e4babf360026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_caption(link, text=\"Image source\"):\n",
    "    t = f\"\"\"<a href=\"{link}\" target=\"blank_\" style=\"color:#808080\">{text}</a> \"\"\"\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a57f2115-00b9-44eb-9b52-6a4d82f7a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the selected figure\n",
    "\n",
    "def get_figure_selector(figures, headline=None, button_labels=None, ic_label_predictions=None):\n",
    "    if button_labels is None:\n",
    "        button_labels = {str(i):i for i in range(len(figures))}\n",
    "    output = Output()\n",
    "    buttons = [Button(description=str(i)) for i in button_labels.keys()]\n",
    "    prediction_label = Output()\n",
    "    \n",
    "    def show_figure(fig_index):\n",
    "        # Clear the previous output\n",
    "        output.clear_output(wait=True)\n",
    "        with output:\n",
    "            display(figures[fig_index])\n",
    "        if ic_label_predictions:\n",
    "            with prediction_label:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(ic_label_predictions[fig_index]))\n",
    "    \n",
    "    # Define what happens when a button is clicked\n",
    "    def on_button_click(b):\n",
    "        fig_index = button_labels[b.description]\n",
    "        show_figure(fig_index)\n",
    "    \n",
    "    # Attach the click event to each button\n",
    "    for button in buttons:\n",
    "        button.on_click(on_button_click)\n",
    "    \n",
    "    # Display the buttons and the output area\n",
    "    button_box = HBox(buttons)\n",
    "    if headline:\n",
    "        heading = HTML(f\"\"\"<h2>{headline}</h2>\"\"\")\n",
    "    else:\n",
    "        heading = HTML(\"\")\n",
    "    \n",
    "    panel = VBox([heading, button_box, prediction_label, output])\n",
    "    return panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d5f63-14d1-4141-87b6-23b1c3b43c8a",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a7d9380-b543-49bf-8e66-2a638ea508fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84046e99276042c09e64b2f533fc0252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\nWelcome! \\nThis tutorial has two purposes\\n<br>\\n<b>First</b>, it teaches you to w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#raw = None\n",
    "\n",
    "text = \"\"\"\n",
    "Welcome! \n",
    "This tutorial has two purposes\n",
    "<br>\n",
    "<b>First</b>, it teaches you to work with EEG data with explanations and visualizations. \n",
    "<br>\n",
    "<b>Second</b>, it allows you to deeper your understanding by applying the newly learned concepts to real data. \n",
    "<br>\n",
    "You will see panels with the heading <b>Try it yourself!</b> where you can visualize and manipulate data in real time. \n",
    "Working with data on your own will help you learn more easily than by just reading explanations. Make use of the interactive panels throughout this tutorial!\n",
    "\"\"\"\n",
    "panel_1 = HTML(text)\n",
    "\n",
    "\n",
    "code = code_block(\"\"\"\n",
    "x = mne.call_function(x)\n",
    "\"\"\")\n",
    "text = f\"\"\"\n",
    "Within this course, we will use a python-library called MNE-python from time to time. \n",
    "MNE-python provides lots of functions you can use to work with EEG data. \n",
    "When we use some code from MNE, it will be displayed like this: \n",
    "{code}\n",
    "\n",
    "This is not a full tutorial on MNE though. If you want to learn more about MNE, you can browse their tutorials by clicking <a href=\"https://mne.tools/stable/documentation/index.html\" target=\"_blank\"><u>here</u></a>.\n",
    "If you don't care about coding and using MNE, you can also ignore the code examples. You will still be able to understand what is happening.\n",
    "\"\"\"\n",
    "\n",
    "panel_3 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "The course is structured as follows:\n",
    "<ul>\n",
    "<li> Chapters (1) and (2) explain <b>how</b> an EEG works, how the data recorded with an EEG looks like and introduces the concept of <b>noise</b>.</li>\n",
    "<li> Chapters (3) to (5) go into more detail an how <b>periodic signals</b> can be described and analyzed in general. These chapter are not EEG-specific. </li>\n",
    "<li> Chapter (6) describes the EEG signal using the learnings from (3) to (5) and introduces the common <b>eeg bandwidths</b>. </li>\n",
    "<li> Chapter (7) introduces the <b>power spectrum</b>, which is a more sophisticated way to analyze and understand EEG data.</li>\n",
    "<li> Chapter (8) explains how to <b>filter</b> eeg data to remove noise.</li>\n",
    "<li> Chapter (9) explains why eeg data always needs to be compared to a <b>reference</b>.</li>\n",
    "<li> Chapter (10) explains what <b>events</b> are.</li>\n",
    "<li> Chapter (11) deepen the understanding of the <b>noise</b> concept and explain common <b>artifacts</b> that can be found in eeg data.</li>\n",
    "<li> Chapters (12) and (13) explain the <b>Independent Component Analysis (ICA)</b> and educate the reader on identifying <b>artifacts</b> with it.</li> \n",
    "</ul>\n",
    "\"\"\"\n",
    "panel_4 = HTML(text)\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_3, panel_4], [\"1\",\"2\",\"3\", \"4\"])\n",
    "display(chapter) #../sub400/400MNE.set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f3f72-7cc1-4378-b2a6-4f0156687e76",
   "metadata": {},
   "source": [
    "# (1) What is an EEG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd76fca0-ec12-4700-913d-52c62cced374",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301fffdad5aa4eefb6508055f1ad3f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value='\\nWhat is an EEG at all? EEG stands for Electroencephalography. \"Ence…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "What is an EEG at all? EEG stands for Electroencephalography. \"Encephalon\" is the greek word for brain and \"graphein\" means writing. \n",
    "Accordingly, an EEG is a device that writes electric brain signals.\n",
    "You may already know, that the brain sends electric signals along the neurons' axons. \n",
    "There are plenty of neurons and they send many tiny signals per second. \n",
    "If researchers want to measure these signals, there are different ways to do that, each of which has advantages and disadvantages. \n",
    "If you want to measure the signal very precisely on the basis of single neurons, you have to cut open the head and implement tiny measurement tools. \n",
    "This is done in animals most often, because human's don't want their heads to be cut open. \n",
    "The EEG however, doesn't need any operation beforehand, which is why we call it a <b>non-invasive method</b>. \n",
    "You can easily imagine that this makes it much more accessible for research. \n",
    "You just need to place an eeg cap on the participants head that looks like this:\n",
    "\"\"\"\n",
    "im_eeg_hood = Image(value=open(\"resources/EEG_cap.jpg\", \"rb\").read(),width=200,height=250)\n",
    "image_caption = get_image_caption(\"https://en.wikipedia.org/wiki/Electroencephalography#/media/File:EEG_cap.jpg\")\n",
    "panel_1 = VBox([HTML(text), im_eeg_hood, HTML(image_caption)])\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "I already mentioned, that the EEG measures electric signals of neurons, and it does that from outside of the skull. \n",
    "If a single neurons sends an electric signal, we would never be able to record this from the outside of the skull, though, because it would be way too small. \n",
    "However, if many neurons send electric signals at the same time, they sum up and this can be measured from the outside with sensitive electrodes. \n",
    "This is what the EEG does. \n",
    "It is important to understand, that the EEG doesn't record <i>all</i> the brain activity but just the very strong highlights. \n",
    "<br>\n",
    "Imagine you are standing outside of a soccer arena. \n",
    "You don't see the game, you just hear the fans' voices. \n",
    "Would you be able to understand everyting that happens within the game?\n",
    "Most likely not. But you would be able to recognize major events like a scored goal, because here all fan's (or half of them) cheer altogether. \n",
    "Transferred to the brain, these are the kinds of signals an EEG measures. \n",
    "\"\"\"\n",
    "panel_2 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "On the picture shown before you may have recognized, that an EEG cap includes multiple electrodes at different locations. \n",
    "The brain has a very sophisticated structure and different parts of it are responsible for different functions. \n",
    "For example (and strongly simplified), the back part of the prain (the so-called occpital region) is associated with processing visual information, while the frontal part is responsible for deliberate and conscious thought. \n",
    "Due to that structure, it makes sense to place electrodes at different locations on the skull to measure these differences. \n",
    "<br>\n",
    "Additionally, multiple electrodes allow to measure a signal more reliably. \n",
    "In science it is common to use many measurements and aggregate them to get a more robust picture of what is happening. \n",
    "If you were to determine the depth of the ocean you also would perform multiple measurements at different locations, wouldn't you?\n",
    "\"\"\"\n",
    "panel_3 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "How many electrodes does an EEG have at all and where exactly are they located? \n",
    "There is not <i>the one and only</i> eeg cap, but there are different standards on the number and location of electrodes. \n",
    "The format used most often is known as 10-20-system. In the following you see a visualization of the electrodes.\n",
    "\"\"\"\n",
    "im_10_20 = Image(value=open(\"resources/10_20.png\", \"rb\").read(),width=400,height=600)\n",
    "text_2 = f\"\"\"\n",
    "{get_image_caption(\"https://en.wikipedia.org/wiki/10%E2%80%9320_system_%28EEG%29#/media/File:21_electrodes_of_International_10-20_system_for_EEG.svg\")}\n",
    "<br>\n",
    "Do you see the nose at the top (Nasion) and the ears to the left and right? The person in the image looks upwards and you look at their head from above. \n",
    "You see that the electrodes all have names like NZ, PO3 or P9. \n",
    "When you look at the data recorded by the EEG, you want to know which electrode captures which signal, and this is why you want to be familiar with the naming scheme.\n",
    "The first letter(s) indicate the region of the brain the electrode is placed on: pre-frontal (Fp), frontal (F), temporal (T), parietal (P), occipital (O) and central (C). \n",
    "This indicates the position on the axis from nose to back of the head. \n",
    "On top of that, there is information on the location of the axis from ear to ear. \n",
    "Electrodes with a \"Z\" are placed in the middle of the brain. \n",
    "Electrodes with even numbers are placed on the right part of the head, while electrodes with uneven numbers are on the left part. \n",
    "For example, FZ is frontal and in the middle. P4 is parietal and on the right part. C3 is central and on the left part of the brain. \n",
    "You also see two electrodes \"A1\" and \"A2\" placed behind the ear. Sometimes they are also called \"M1\" and \"M2\" instead. \n",
    "In some picture there are also two points NZ and IZ. These are <b>not</b> electrodes, but just points of reference, that are used, when the EEG cap is placed on a person's skull. \n",
    "While they may appear on such visualizations, you will never find EEG data recorded from electrodes called NZ or IZ. \n",
    "\"\"\"\n",
    "panel_4 = VBox([HTML(text), im_10_20, HTML(text_2)])\n",
    "\n",
    "text = \"\"\"\n",
    "The 10-20 system we just saw had 21 electrodes in total. \n",
    "However, sometimes you don't need that much, or you need even more. Hence, different standards exist with different numbers of electrodes. \n",
    "Often you may see the 10-10 system, which is an extension of the 10-20 system. If you observe it carefully, you will see that it includes all the electrodes of the 10-20 system, extended by some more:\n",
    "\"\"\"\n",
    "im_10_10 = Image(value=open(\"resources/10_10.png\", \"rb\").read(),width=400,height=600)\n",
    "\n",
    "text_2 = f\"\"\"\n",
    "{get_image_caption(\"https://de.wikipedia.org/wiki/Datei:International_10-20_system_for_EEG-MCN.png\")}\n",
    "<br>\n",
    "Other EEG caps may use less electrodes like the 14 ones here:\n",
    "\"\"\"\n",
    "im_14 = Image(value=open(\"resources/14_channel_eeg.png\", \"rb\").read(),width=400,height=600)\n",
    "\n",
    "text_3 = f\"\"\"\n",
    "{get_image_caption(\"https://www.researchgate.net/figure/Location-of-14-electrodes-of-Emotiv-EEG-device_fig1_283441559\")}\n",
    "<br>\n",
    "When working with EEGs, it is important to decide on the number of electrodes before, depending on the equipment available and the researchs' demands. \n",
    "Even more important is to know which system has been used when interpreting the data, though. \n",
    "When you analyze EEG signals (as we will do in this tutorial later), you need to know how many electrodes recorded them and where exactly these electrodes have been placed. \n",
    "\"\"\"\n",
    "panel_5 = VBox([HTML(text), im_10_10, HTML(text_2), im_14, HTML(text_3)])\n",
    "\n",
    "montages = get_builtin_montages()\n",
    "montages = \"\".join([f\"<li>{x}</li>\" for x in montages])\n",
    "channels = np.random.choice(raw.ch_names, size=min(6, len(raw.ch_names)))\n",
    "code = code_block(\"\"\"\n",
    "raw.set_montage(make_standard_montage(\"standard_1020\"))\n",
    "raw.plot_sensors(ch_type=\"eeg\")\n",
    "\"\"\")\n",
    "text = f\"\"\"\n",
    "MNE comes with a list of builtin channel systems (also called <b>montages</b>) that are the following:\n",
    "<br>\n",
    "<ul>\n",
    "{montages}\n",
    " </ul>\n",
    "<br>\n",
    "Let's assume your data is missing the information on which system has been used exactly. However, you still have the channel names and from these, the system may be inferred. \n",
    "Here are some channels of your data:\n",
    "{channels}\n",
    "Can you already guess, which system has been used?\n",
    "MNE also provides a function to apply a selected montage to a dataset. \n",
    "{code}\n",
    "Try it with one of the montages listed above!\n",
    "Can you find the right montage for your data?\n",
    "What happens, if you use a montage that doesn't fit? \n",
    "\"\"\"\n",
    "text_input = Text(placeholder=\"Montage name\")\n",
    "def try_out_montage(name=None):\n",
    "    if name is None:\n",
    "        name = text_input.value\n",
    "    try:\n",
    "        x = raw.copy()\n",
    "        x.set_montage(make_standard_montage(name))\n",
    "        t = \"This was sucesfull! This is where the electrodes are located on the skull:\"\n",
    "        return [HTML(t), x.plot_sensors(ch_type=\"eeg\", show=False)]\n",
    "    except Exception as e:\n",
    "        t = \"Oh no, there was en error! Do you understand the error message? What was the problem?\"\n",
    "        return (HTML(t),str(e))\n",
    "output_montage = Output()\n",
    "def display_montage(b):\n",
    "    t, montage_or_error = try_out_montage()\n",
    "    with output_montage:\n",
    "        clear_output()\n",
    "        display(t,montage_or_error)\n",
    "button = Button(description=\"Apply Montage\")\n",
    "button.on_click(display_montage)\n",
    "\n",
    "panel_6 = VBox([HTML(text), text_input, button, output_montage])\n",
    "\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4, panel_5, panel_6], [\"1\",\"2\",\"3\",\"4\", \"5\", \"Try it yourself!\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4484e6-e07b-4542-a4fa-9e5359f02642",
   "metadata": {},
   "source": [
    "# (2) EEG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad35c213-349a-4c56-99e9-c08c9a4db1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70198efd1eff4f1086f279319c663bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value='\\nNow that we know what an EEG measures, it is time to take a look at…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Now that we know what an EEG measures, it is time to take a look at the data it collects. \n",
    "Most of the time, you will see the data being displayed like this: \n",
    "\"\"\"\n",
    "\n",
    "im_eeg = Image(value=open(\"resources/eeg_raw_signal.png\", \"rb\").read(),width=400,height=600)\n",
    "\n",
    "text_2 = \"\"\"\n",
    "First of all, you see multiple lines of data here. \n",
    "These are called <b>channels</b> and corespond to the different electrodes you are already familiar with. \n",
    "You see the electrode/channel names on the left.\n",
    "You also see, that the data is ploted over time on the x-axis. \n",
    "With an eeg, you don't just record brain activity at a given point in time, but you record it over a longer timerange. \n",
    "The y-axis displays the power of the electric activity measured in volt.\n",
    "However, as the brain uses rather low voltages (compared to the electric current coming out of your power socket), eeg data will typically be displayed in μV (micro-volt, i.e. 10⁻⁶ volt).\n",
    "\"\"\"\n",
    "\n",
    "panel_1 = VBox([HTML(text), im_eeg, HTML(text_2)])\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "One thing to be aware of when working with EEG data is the fact, that this data is very noisy. What exactly does that mean? \n",
    "Take a look at the picture below. It is quite hard to recognize anything in it, right? \n",
    "That is because many pixels with random colors have been added to the original image. That is noise. \n",
    "However, observe what happens when you reduce the noise by dragging the slider to the left. Can you recognize the image now and tell, what it shows?\n",
    "\n",
    "\"\"\"\n",
    "def add_gaussian_noise(image_array, mean=0, std=250):\n",
    "    # Generate Gaussian noise\n",
    "    gauss = np.random.normal(mean, std, image_array.shape)\n",
    "    noisy_image = image_array + gauss\n",
    "    \n",
    "    # Clip values to stay in valid range (0, 255)\n",
    "    noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return noisy_image\n",
    "    \n",
    "output_noisy_image = Output()\n",
    "image_otter = PIL.Image.open('resources/otter.jpg').convert('RGB')\n",
    "image_array = np.array(image_otter)\n",
    "noise_slider = FloatSlider(min=0, max=500, value=500, readout=False, continous_update=False)\n",
    "def on_value_change(change):\n",
    "    noisy_image_array = add_gaussian_noise(image_array, std=noise_slider.value)\n",
    "    noisy_image = PIL.Image.fromarray(noisy_image_array)\n",
    "    with output_noisy_image:\n",
    "        clear_output(wait=True)\n",
    "        display(noisy_image)\n",
    "        \n",
    "on_value_change(None)      \n",
    "noise_slider.observe(on_value_change, names=\"value\")\n",
    "panel_2 = VBox([HTML(text),noise_slider, output_noisy_image])\n",
    "\n",
    "text = \"\"\"\n",
    "In the previous tab you saw, that noise can make it hard to recognize an image. \n",
    "However, noise can also be applied to different kinds of data. For example, noise in a sound recording can make it hard to hear what a person is saying.\n",
    "Likewise, eeg data, which is electric voltage over time can become noisy, as you see in the following. \n",
    "The first plot is a pure signal, the second plot is the same signal but with some noise added on top. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "text_2= \"\"\"Do you see the first curve hidden in the second?\n",
    "<br>\n",
    "In general, we say that <b>noise</b> superimposes a <b>signal</b>.\n",
    "The signal is always the data you <b>want</b> to measure, while the noise is everything else that distorts it. \n",
    "In the previous tab, the image of the otter was the signal and in the above figure, the first curve is the signal.\n",
    "In an EEG, brain activity is the signal we are interested in and everything else we measure, that is not coming from the brain, is noise. \n",
    "For example, muscle movements also procude electric activity that may be recorded by the EEG. \n",
    "<br>\n",
    "However, you don't need to be completely noise-free all the time. With a little noise you still had been able to recognize the image, right? \n",
    "If the data is too noisy, we may not be able to detect the actual signal, but we can afford having some noise. If the signal is strong enough, it will still be visible through the noise.\n",
    "In a later chapter, we will see how certain kinds of noise can be filtered from the data to make the signal shine through. \n",
    "\"\"\"\n",
    "\n",
    "im_noisy_signal = Image(value=open(\"resources/noisy_signal.png\", \"rb\").read(),width=width,height=height)\n",
    "\n",
    "\n",
    "panel_3 = VBox([HTML(text), im_noisy_signal, HTML(text_2)])\n",
    "\n",
    "text = \"\"\"This is how some real eeg data looks like.\n",
    "Note that you can interact with this visualization. You can click on the bars on the x and y axis to jump to different points in time or different channels.\n",
    "Do the signals differ between the channels? Are there channels that stand out in particular?\n",
    "Can you use your knowledge about the electrodes' locations to explain some of these differences?\"\"\"\n",
    "\n",
    "\n",
    "output_raw_eeg = Output()\n",
    "plt.clf()\n",
    "with output_raw_eeg:\n",
    "    raw.plot(n_channels=4)\n",
    "\n",
    "panel_4 = VBox([HTML(text), output_raw_eeg])\n",
    "\n",
    "text = \"\"\"\n",
    "You can read eeg data as follows in MNE: \n",
    "\"\"\"\n",
    "\n",
    "code = code_block(\"\"\"\n",
    "    from mne.io import read_raw_eeglab, read_raw\n",
    "    raw = read_raw(\"path/to/your/dataset\")\n",
    "\"\"\", \n",
    "                 links = {\"read_raw\":\"https://mne.tools/dev/generated/mne.io.read_raw.html\"})\n",
    "\n",
    "text2 = \"\"\"\n",
    "For displaying the data, you can use the plot function:\n",
    "\"\"\"\n",
    "code2 = code_block(\"\"\"raw.plot(n_channels=4)\"\"\", links={\"plot\":\"https://mne.tools/dev/generated/mne.io.Raw.html#mne.io.Raw.plot\"})\n",
    "\n",
    "text3 = \"\"\"\n",
    "A more detailed overview on how to read data from different EEG devies in MNE can be found <a href=\"https://mne.tools/dev/auto_tutorials/io/20_reading_eeg_data.html#sphx-glr-auto-tutorials-io-20-reading-eeg-data-py\" target=\"_blank\"><u>here</u></a>.\n",
    "\"\"\"\n",
    "\n",
    "code_panel = VBox([HTML(text), code, HTML(text2), code2, HTML(text3)])\n",
    "\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4, code_panel], [\"1\",\"2\",\"3\", \"Try it yourself!\", \"Code\"])\n",
    "display(chapter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8dc18-7cd4-4e9d-aeaf-4400a4db79e3",
   "metadata": {},
   "source": [
    "# (3) What is a periodic signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "be85248d-4973-4078-b263-c6795b70e320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0b5a650efb479982fda693adf05b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value='\\nWe have taken a first look at EEG data in the previous chapter. \\nN…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "im_string_moving = Image(value=open(\"resources/string_moving.gif\", \"rb\").read(),width=width,height=height)\n",
    "\n",
    "text = \"\"\"\n",
    "We have taken a first look at EEG data in the previous chapter. \n",
    "Now we want to understand its structure in more detail. \n",
    "To this end, we first need to take a step back and talk about what <b>periodic signals</b> are in general. \n",
    "In many applications you have processes that are repeating. \n",
    "For example, if you pull the string of a guitar and let it go, this string will move back and forth in a periodic manner.\n",
    "You see a visualization of this process in the following. \n",
    "The string starts at the top (a), then it moves back to the middle (e) and continues moving in the opposite direction (i). \n",
    "Then it moves back to the middle again (m) and back to the position it started from (p and ultimatively a). From here it moves to the middle again and so on. \n",
    "\"\"\"\n",
    "\n",
    "text_2 = f\"\"\"\n",
    "{get_image_caption(\"https://www.phys.unsw.edu.au/jw/strings.html\")}\n",
    "<br>\n",
    "Now imagine you are able to measure the position of middle point of the string at different points in time during this process. \n",
    "Let's say the maximum displacement of the spring towards the top (as in (a)) is indicated by a value of 1 and the middle position of the string (e) is a 0. \n",
    "The maximum displacement in the opposite direction (as in (i) is a -1.\n",
    "If we plot the time on the x-axis and the displacement on the y-axis, we would get data points like these: \n",
    "\"\"\"\n",
    "\n",
    "x = [(1/2)*np.pi, (3/4)*np.pi, np.pi, (5/4)*np.pi, (3/2)*np.pi, (7/4)*np.pi, 2*np.pi]\n",
    "y = np.sin(x)\n",
    "fig,ax = plt.subplots()\n",
    "ax.set_xticks(x, labels=[\"a\", \"c\", \"e\", \"g\", \"i\", \"k\", \"m\"])\n",
    "ax.scatter(x,y, color=\"black\")\n",
    "fig.set_size_inches(4,2)\n",
    "output_sampling_points = Output()\n",
    "with output_sampling_points:\n",
    "    display(fig)\n",
    "\n",
    "text_3 = \"\"\"\n",
    "If we do this more frequently and with smaller timesteps in between, the curve looks more smooth:\n",
    "\"\"\"\n",
    "\n",
    "output_smooth_sampling = Output()\n",
    "x = np.arange((1/2)*np.pi, 2*np.pi, 0.1)\n",
    "y = np.sin(x)\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x,y, color=\"black\")\n",
    "ax.set_xticks([])\n",
    "fig.set_size_inches(4,2)\n",
    "with output_smooth_sampling:\n",
    "    display(fig)\n",
    "\n",
    "panel_1 = VBox([HTML(text), im_string_moving, HTML(text_2), output_sampling_points, HTML(text_3), output_smooth_sampling])\n",
    "\n",
    "text = \"\"\"\n",
    "Now we saw a moving string turn into a <b>periodic signal</b>. \n",
    "This was possible, because it's movemenet was periodic, i.e. recurring over time. \n",
    "Likewise, brain activity can be periodic. Say you measure the activity over time and see that it goes up and down again and again. \n",
    "Can you imagine that such a signal would look like this?\n",
    "\"\"\"\n",
    "output_ideal_signal = Output()\n",
    "x = np.arange((1/2)*np.pi, 12*np.pi, 0.1)\n",
    "y = np.sin(x)\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(x,y, color=\"black\")\n",
    "fig.set_size_inches(4,2)\n",
    "with output_ideal_signal:\n",
    "    display(fig)\n",
    "text2 = \"\"\"\n",
    "You may wonder, why we need to describe brain activity as a periodic signal. \n",
    "In the next chapters you will understand, that this representation of the data allows us to describe and manipulate the data more easily. \n",
    "\"\"\"\n",
    "panel_2 = VBox([HTML(text), output_ideal_signal, HTML(text2)])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2], [\"1\", \"2\"])\n",
    "display(chapter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b425b-174a-454c-a4a1-6b6a7e40e814",
   "metadata": {},
   "source": [
    "# (4) Signal Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "618e8277-de11-4ca9-80bd-41b84575d884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a98a73fbe646259dc94ddcf8f166e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\nIn the previous chapter we understood what a periodic signal is and why brain acti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "im_1_5 = Image(value=open(\"resources/amp_1_freq_5.png\", \"rb\").read(),width=width,height=height)\n",
    "im_1_25 = Image(value=open(\"resources/amp_1_freq_25.png\", \"rb\").read(),width=width,height=height)\n",
    "im_7_5 = Image(value=open(\"resources/amp_7_freq_5.png\", \"rb\").read(),width=width,height=height)\n",
    "im_7_25 = Image(value=open(\"resources/amp_7_freq_25.png\", \"rb\").read(),width=width,height=height)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text1 = \"\"\"\n",
    "In the previous chapter we understood what a periodic signal is and why brain activity can be interpreted as such. \n",
    "We now want to discuss some properties that describe signals that are the <b>frequency</b> and the <b>amplitude</b>.\n",
    "<br>\n",
    "The <b>frequency</b> indicates, how often a signal oscillates, i.e., how often it goes up and down in a given time-range. \n",
    "We measure the frequency in Hertz (Hz), which can be understood as the number of ups and downs per second. \n",
    "A frequency of 50Hz means 50 ups and downs in one second. \n",
    "You can use the frequency to describe different processes that repeat over time. \n",
    "For example, you heart beats ~60 times a minute, which is a frequency of 0.016Hz. \n",
    "If you play the A-key on a piano, the string and the air it puts into motion will oscilate with a frequency of 440Hz. \n",
    "If you play a higher note, the frequency increases and with lower notes, the frequency decreases. \n",
    "Electromagentic waves have frequencys as well. Red light has a frequency of 400THz (terra herz, that is 10¹² herz) and X-rays have a frequency around 10¹⁸ Hz. \n",
    "\"\"\"\n",
    "\n",
    "panel_1 = HTML(text1)\n",
    "\n",
    "text = \"\"\"\n",
    "As mentioned, the frequency in Hz is the number of oscilations per second. In the following you see two signals. \n",
    "\"\"\"\n",
    "answer_text = \"\"\"\n",
    "The left signal has a frequency of 5 Hz. You can find out by counting how often the signal does a full oscilation from 0 (where it starts), up to 1, down to -1 and back to 0.\n",
    "You could also count the number of mountains (+1) or valleys (-1) in 1 second. \n",
    "The right signal has a higher frequency of 25 Hz. \n",
    "\"\"\"\n",
    "answer = get_answer_to_reveal(\"Can you tell, which frequencies they have?\", answer_text)\n",
    "panel_2 = VBox([HTML(text), HBox([im_1_5, im_1_25]), answer])\n",
    "\n",
    "text = \"\"\"\n",
    "The <b>amplitude</b> tells you, how much a signal goes up and down in one cycle. \n",
    "In the following you see two plots of same frequency but with different amplitudes. Do you see, that the one covers a range from -1 to +1 on the y-axis, but the other covers a bigger range from -7 to +7?\n",
    "The first signal has an amplitude of 1, the second has an amplitude of 7. \n",
    "\"\"\"\n",
    "text_2 = \"\"\"\n",
    "The amplitude of a signal is related to its energy or power (which will be explained later in more detail). \n",
    "If you play a sound on an instrument, higher amplitude means a louder sound. \n",
    "If you make yourself aware that a very loud noise can even physically harm your ear, you understand, that this is related to higher energy. \n",
    "\"\"\"\n",
    "panel_3 = VBox([HTML(text), HBox([im_1_5,im_7_5]), HTML(text_2)])\n",
    "\n",
    "text = \"\"\"\n",
    "Here you can create your own signals. \n",
    "You can control the frequency and the amplitude with the sliders and see how this affects the curve. \n",
    "\"\"\"\n",
    "x = np.arange(0, 1, 0.001)\n",
    "frequency_slider = FloatSlider(min=0, max=30, value=5, description=\"Frequency\")\n",
    "amplitude_slider = FloatSlider(min=0, max=10, value=2, description=\"Amplitude\")\n",
    "signal_plot = Output()\n",
    "def show_signal(b):\n",
    "    frequency = frequency_slider.value\n",
    "    amplitude = amplitude_slider.value\n",
    "    y = amplitude*np.sin(frequency*x*2*np.pi)\n",
    "    with signal_plot:\n",
    "        clear_output(wait=True)\n",
    "        fig,axs = plt.subplots()\n",
    "        axs.plot(x,y)\n",
    "        axs.set_ylim(-10,10)\n",
    "        axs.set_xlim(0,1)\n",
    "        display(fig)\n",
    "create_plot_button = Button(description=\"Create Signal\")\n",
    "create_plot_button.on_click(show_signal)\n",
    "panel_4 = VBox([HTML(text), HBox([frequency_slider, amplitude_slider, create_plot_button]), signal_plot])\n",
    "\n",
    "im_4_15 = Image(value=open(\"resources/amp_4_freq_15.png\", \"rb\").read(),width=width,height=height)\n",
    "answer = get_answer_to_reveal(\"What is the frequency and the amplitude of this signal?\", \"Amplitude: 4 (because the signal goes from -4 to 4 on the y-axis). Frequency: 15 (because there are 15 ups and downs within one second).\")\n",
    "panel_5 = VBox([im_4_15, answer])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4, panel_5], [\"1\",\"2\", \"3\", \"Try it yourself!\", \"Test yourself!\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209a6e4-3f0f-4a7e-8a9c-e2e50c167e49",
   "metadata": {},
   "source": [
    "# (5) Sampling Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fe9c9be6-23a6-4548-826a-6b00160e87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db517c415b5444ea733603b03d5af38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value=\"\\nWe just learned, that an EEG measure brain activity as an electric signal and we s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "file = open(\"resources/10_hz_signal.png\", \"rb\")\n",
    "\n",
    "text = \"\"\"\n",
    "We just learned, that an EEG measure brain activity as an electric signal and we saw, that such signals have a frequency and an amplitude. \n",
    "We now want to explain the process of sampling this data. \n",
    "Brain activity is continous, that means, it happens all the time. \n",
    "However, the EEG can't measure the activity <i>all the time</i> but does a measurement every few timesteps. \n",
    "For comparison, think of making a video with your camera. The camera also doesn't record <i>every</i> movement that happens, but does ~30 fotographs per second. \n",
    "This is enough to capture a typical scenery and display a fluid movement. \n",
    "However, there can be scenarios where 30 fotographs per second is not sufficient. \n",
    "If you want to record an explosion in detail, you may need a slow-motion camera that takes more images per second. \n",
    "<br>\n",
    "With this idea in mind, let us see how in the eeg the <b>sampling frequency</b>, which is the analogon to the camera's images per second, can influence the data we record.\n",
    "\"\"\"\n",
    "panel_0 = HTML(text)\n",
    "\n",
    "text1 = \"Say we have a signal that looks like this.\"\n",
    "im_signal = Image(value=open(\"resources/10_hz_signal.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_1 = VBox([HTML(text1), im_signal])\n",
    "\n",
    "text2 = \"We now sample this signal, i.e., we measure it's vaue at different points in time. These points in time we see as orange dots here. All the points have equal distance on the x-axis. If we do that often enough, we can connect the dots and get the signal.\"\n",
    "im_signal_sampled = Image(value=open(\"resources/10_hz_signal_sampled.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_2 = VBox([HTML(text2), im_signal_sampled])\n",
    "\n",
    "text3 = \"However, what would happen, if we use very few points only? Take a look at the following example. Again, these points all have the same distance on the x-axis. However, they don't allow to capture the full range of the signal. If we connect them, we get a signal (flat orange line) that is very different from the one, we wanted to sample.\"\n",
    "im_signal_sampled_incorrectly_1 = Image(value=open(\"resources/10_hz_signal_sampled_incorrectly_1.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_3 = VBox([HTML(text3), im_signal_sampled_incorrectly_1])\n",
    "\n",
    "text4 = \"It would be very bad luck, if all our points catch the exact same value of the signal, as in the previous example. However, if the number of points is too small, we can end up with arbitrary signals, that just don't reflect the signal we are sampling from\"\n",
    "im_signal_sampled_incorrectly_2 = Image(value=open(\"resources/10_hz_signal_sampled_incorrectly_2.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_4 = VBox([HTML(text4), im_signal_sampled_incorrectly_2])\n",
    "\n",
    "text = \"\"\"\n",
    "In a similar way, the shift between the signal and the sampling points can have an influence on the signal reconstructed from the sampling. \n",
    "For example, you could sample your data at timepoints [0.1,0.2,0.3,0.4,0.5] or at [0.15, 0.25, 0.35, 0.45, 0.55]. \n",
    "Note that in both cases, the distance between the sampling points (i.e., the sampling frequency) is the same, but the second row of sampling points is shifted towards the first. \n",
    "Such a shift can lead to a different signal, as visualied in the following. In both plots, the same sampling frequency is used, but the second one is shifted a little in time.\n",
    "\"\"\"\n",
    "\n",
    "im_shift_0 = Image(value=open(\"resources/nyquist_shift_0.png\", \"rb\").read(),width=width,height=height)\n",
    "im_shift_1 = Image(value=open(\"resources/nyquist_shift_1.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_5 = VBox([HTML(text), HBox([im_shift_0, im_shift_1])])\n",
    "\n",
    "text_explanation = \"\"\"We just saw, that having too few sampling points can be problematic. \n",
    "The number of sampling points in a given time-range is called the sampling frequency. \n",
    "E.g., if we collect 10 points per second, this is a sampling frequency of 10Hz.\n",
    "If we have a signal of n Hz, the sampling frequency must be at least 2*n Hz to capture the signal. \n",
    "This is called the <b>Nyquist Frequency</b>. \n",
    "The other way round, if you sample with a frequency of n Hz, you can not expect to capture signals that have a frequency of more than n/2 Hz.\n",
    "<br>\n",
    "In practice, you might want to relax the threshold of the nyquist frequency a little. \n",
    "As you have seen, there are additional influences on the sampling (such as the shift we talked about). \n",
    "To account for these you want to have higher sampling rate than indicated by the Nyquist frequency. \n",
    "Say you expect a signal of 40Hz. While in theory 80Hz as sampling frequency should be enough, you may want to use a sampling frequency of 100Hz to be more robust against other influences. \n",
    "In general, 2.5*n (instead of 2*n) is a reasonable rule of thumb. \n",
    "<br>\n",
    "As a consequence, you need to be aware of the Sampling Frequency when you analyze your EEG data. \n",
    "If you sampled it with 60 Hz, you can not expect any frequencies higher than 30Hz. \n",
    "In the next chapter, we will see which frequencies are typically expected in brain activity. \n",
    "If you want to record those, you need to make sure that your sampling frequency is high enough to allow that. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "panel_6 = HTML(text_explanation)\n",
    "\n",
    "shift = FloatSlider(min=0, max=0.1, value=0, step=0.002, description=\"Shift\")\n",
    "output_sampling_plot = Output()\n",
    "def sample(sampling_freq=50):\n",
    "    x = np.arange(0, 1, 0.001)\n",
    "    # create a 20hz signal\n",
    "    y = np.sin(20*x*2*np.pi)\n",
    "    x_sample = np.arange(0,1, 1/sampling_freq) + shift.value\n",
    "    # sample the 20hz signal with the given sampling_freq\n",
    "    sample = np.sin(20*x_sample*2*np.pi)\n",
    "    \n",
    "    fig,axs = plt.subplots(1,2,sharex=True, sharey=True)\n",
    "    axs[0].plot(x,y, color=\"black\")\n",
    "    axs[0].scatter(x_sample, sample, color=\"orange\")\n",
    "    axs[1].plot(x_sample, sample, color=\"orange\")\n",
    "    fig.set_size_inches(14,6)\n",
    "    return fig\n",
    "\n",
    "def show_sampling_plot(b,sampling_freq=None):\n",
    "    if sampling_freq is None:\n",
    "        sampling_freq = int(input_freq.value)\n",
    "    fig = sample(sampling_freq)\n",
    "    \n",
    "    with output_sampling_plot:\n",
    "        clear_output()\n",
    "        display(fig)\n",
    "\n",
    "show_sampling_plot(None, sampling_freq=50)\n",
    "text = \"\"\"You can try it yourself! The signal you see here has a frequency of 20Hz. What happens if you sample it with lower or higher frequencies?\n",
    "You can also shift the sampling points with the slider. How does that affect the reconstructed signal? Does the signal become more robust against shifting, when you use a higher sampling frequency?\n",
    "\"\"\"\n",
    "input_freq = Text(placeholder=\"50\")\n",
    "button = Button(description=\"Sample\")\n",
    "button.on_click(show_sampling_plot)\n",
    "panel_7 = VBox([HTML(text), HBox([input_freq, shift, button]), output_sampling_plot])\n",
    "\n",
    "\n",
    "test_input_field = Text()\n",
    "answer = get_answer_to_reveal(\"You are sampling a signal with a frequency of 80Hz. What is the maximum frequency you can expect in the signal you construct from the sampling points?\", \"40 Hz in theory. In practice, you might want to be more conservative and expect not more than 32Hz.\")\n",
    "panel_8 = HBox([test_input_field, answer])\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "MNE allows to change the sampling frequency of a given dataset. This can be used to lower the sampling frequency in the data, but be aware that you can not set it higher than the frequency the data was sampled with in reality.\n",
    "\"\"\"\n",
    "code = code_block(\"\"\"\n",
    "    raw_resampled = raw.resample(<sampling_frequency>)\n",
    "    \"\"\", links={\"resample\":\"https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.resample\"})\n",
    "\n",
    "code_panel = VBox([HTML(text), code])\n",
    "\n",
    "chapter = get_chapter([panel_0, panel_1, panel_2, panel_3, panel_4, panel_5, panel_6, panel_7,panel_8, code_panel], [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",  \"Explanation\", \"Try it yourself\", \"Test yourself!\", \"Code\"])\n",
    "\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ce854-3967-462d-8094-ee64da702425",
   "metadata": {},
   "source": [
    "# (6) EEG bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "72bc1fd0-cb70-4b67-8939-95bfbd8f174b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4107959e624c2192f28feb95b05bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='In the following, we will learn what are typical frequencies for EEG data. We will s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"In the following, we will learn what are typical frequencies for EEG data. We will see that there are different bandwidths, that are associated with different kinds of brain activity.\"\n",
    "panel_1 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "What range of frequencies do EEG waves occur in? We already saw, that signals in general can vary from very fast (X-rays in petaherz range) to very slow (your pulse with ~1 Hz). \n",
    "The brain does not cover this whole spectrum. One can say, that brain activity is limited to the range of roughly 0.5 to 100Hz.\n",
    "As a convention, EEG signals are sub-divided into five different bandwiths Alhpa, Beta, Gamma, Delta and Theta, which include different frequencies. The main reason for this sub-division is, that these bandwidths are associated with different kinds of activity:\n",
    "<p><b>Delta</b> (less than 4Hz) <br> Delta waves have the lowest frequency and occur during sleep.</p>\n",
    "<p><b>Theta</b> (4-8 Hz) <br> Theta waves appear somewhere in the border between sleep and an awake state. If you daydreaming or doing medidation, theta waves may increase.</p>\n",
    "<p><b>Alpha</b> (8-12Hz) <br> Alpha waves reflect a state of alertnes and being prepared to act. However, they are associated with a rather relaxed state and not so much with concentrating deliberately.</p>\n",
    "<p><b>Beta</b> (13-30Hz) <br> When you are awake and have your eyes open, Beta waves occur. They are associated with conscious activities such as listening or thinking concrentratedly.</p>\n",
    "<p><b>Gamma</b> (greater than 30 Hz) <br> The fast Gamma waves appear, when you need to process multiple sources of information in parallel.</p>\n",
    "\n",
    "You can learn more about bandwiths <a href=\"https://nhahealth.com/brainwaves-the-language/\" target=\"_blank\">here</a>\n",
    "\n",
    "\"\"\"\n",
    "panel_2 = HTML(text)\n",
    "\n",
    "text = \"This is how the bandwidths look like in an EEG signal. Do you agree, that Gamma waves have the highest, and Delta waves have the lowest frequency? Do you think you can specify the correct bandwidth for a given signal? If so, go to the next tab!\"\n",
    "image_source = \"\"\"<a href=\"https://www.researchgate.net/publication/275830679_A_New_EEG_Acquisition_Protocol_for_Biometric_Identification_Using_Eye_Blinking_Signals?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ\" target=\"_blank\">Image source</a>\"\"\"\n",
    "im_frequency_bands = Image(value=open(\"resources/frequency_bands_eeg.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_3 = VBox([HTML(text), im_frequency_bands, HTML(image_source)])\n",
    "\n",
    "text = \"Here are two signals. What bandwidths do they belong to?\"\n",
    "panel_4 = HTML(text)\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4], [\"1\",\"2\", \"3\", \"Test yourself!\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fbfbc-e7c4-4106-b499-3e1176e5f8a2",
   "metadata": {},
   "source": [
    "# (7) Power Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4615140d-53b8-4c51-8d6e-ea3eae9320a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e39f5bd59d4781bda1b2479ff1bd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value='In this chapter, we will introduce the Power Spectrum, which is a use…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#https://neuroimage.usc.edu/forums/t/eeg-power-spectral-density/3634\n",
    "\n",
    "text = \"\"\"In this chapter, we will introduce the Power Spectrum, which is a usefull way to display properties of an EEG signal. \n",
    "For the power spectrum, we have the frequency on the x-axis and the Power on the y-axis. The power is derived from the squared amplitude of the signal. What does that mean? \n",
    "You can understand the amplitude as a measure of the energy. The higher the amplitude, the more energy a signal has. You can easily recognize this, when you hear music. A higher amplitude means a louder sound and that is more energy. \n",
    "You don't need to care why exactly we have to square the amplitude (this is just physics), but be aware that squaring a high amplitude also leads to high power.\n",
    "Summing up, the power is a measure of \"how much energy\" is in a signal. When we plot this power over the frequency, we get an idea, which different components an EEG signal consists of.\n",
    "<br>\n",
    "Let's take a look at such a power spectrum.\n",
    "\"\"\"\n",
    "\n",
    "im_power_spectrum = Image(value=open(\"resources/power_spectrum.png\", \"rb\").read(),width=width,height=height)\n",
    "text_2 = f\"\"\"\n",
    "{get_image_caption(\"https://neuroimage.usc.edu/forums/t/eeg-power-spectral-density/3634\")}\n",
    "<br>\n",
    "Do you see, that there is quite a high power around 10Hz? Looks like the 10Hz waves are contributing more to the overall signal than the 20Hz waves, for example.\n",
    "If you watched the figure carefully, you may have noticed, that the y-axis is not micro-volt squared, but micro-volt squared divided by Hz. This is done to obtain better comparibilty between the different bandwidths. \n",
    "Naturally, waves with higher frequencies contain more energy, which would make it unfair to compare a wave of, say 50Hz with one of 1Hz directly. Dividing by the frequency mitigates this problem. \n",
    "If we divide the micro-volt squared by the frequency, we call this a <b>Power Spectrum Density (PSD)</b>. <br>\n",
    "Sometimes you may also see the y-axis of a power spectrum (density) to be in decibel (db). This is just another way of scaling the power to compare different magnitudes of power more easily.\n",
    "You can still interpret the plots the same way though: Higher values mean more power. \n",
    "\"\"\"\n",
    "\n",
    "panel_1 = VBox([HTML(text), im_power_spectrum, HTML(text_2)])\n",
    "\n",
    "text = \"\"\"Remember the bandwidths we saw in the previous chapter? With the power spectrum, it is quite easy to identify which frequencies are present in an EEG signal. Different areas on the x-axis belong to the different bandwidths.\"\"\"\n",
    "im_sleep_eeg = Image(value=open(\"resources/sleep_eeg.png\", \"rb\").read(),width=width,height=height)\n",
    "im_band_power = Image(value=open(\"resources/band_power.png\", \"rb\").read(),width=width,height=height)\n",
    "text_2 = f\"\"\"\n",
    " {get_image_caption(\"https://link.springer.com/article/10.1007/s11571-020-09639-w\")}\n",
    " <br>\n",
    " If you see which bandwidth are dominant, you can already get an idea of what the person is doing.\n",
    " In the following you see EEGs for people that are awake (top left) or sleeping (remaining three images). Do you see how the alpha waves are spiking in the awake state but decline in the sleep states?\n",
    "Which channels are more prominent during sleep? Does that match what you learned about the bandwidths in the previous chapter?\n",
    "\"\"\"\n",
    "\n",
    "image_caption = get_image_caption(\"https://www.researchgate.net/publication/346510774_Criticality_and_the_role_of_the_connectome_in_shaping_slow_oscillations_in_the_brain_during_deep_sleep?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ\")\n",
    "panel_2 = VBox([HTML(text), im_band_power, HTML(text_2), im_sleep_eeg, HTML(image_caption)])\n",
    "\n",
    "\n",
    "text = \"The power spectrum can be plotted for each channel individually or as an average over each channel.\"\n",
    "answer = get_answer_to_reveal(\"Can you guess, what could be an advantage of showing it for each channel?\", \"If you display each channel, you can detect if some channels behave different than others. There are different reasons for that, but one could be, that a channel captures elictric signals that are not coming from the brain but are noise we don't want.\")\n",
    "panel_3 = VBox([HTML(text), answer])\n",
    "\n",
    "\n",
    "text = \"Take a look at the power spectrum of your own data. Do you observe anything interesting? Is the power rising or falling for higher frequencies? Are there any frequencies that are dominant?\"\n",
    "psd = raw.compute_psd(fmax=120)\n",
    "psd_plot = psd.plot(show=False)\n",
    "output_power_spectrum = Output()\n",
    "with output_power_spectrum:\n",
    "    display(psd_plot)\n",
    "panel_4 = VBox([HTML(text),output_power_spectrum])\n",
    "\n",
    "text = \"\"\"\n",
    "You can create the PSD plot from raw data in MNE as follows: \n",
    "\"\"\"\n",
    "code = code_block(\"\"\"\n",
    "psd = raw.compute_psd()\n",
    "psd.plot()\n",
    "\"\"\",\n",
    "                 links = {\"compute_psd\":\"https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.compute_psd\", \"plot\":\"https://mne.tools/stable/generated/mne.time_frequency.Spectrum.html#mne.time_frequency.Spectrum.plot\"})\n",
    "text2 = \"\"\"\n",
    "A tutorial that uses the PSD plot for inspecting the effects of filtering and resampling can be found <a href=\"https://mne.tools/stable/auto_tutorials/preprocessing/30_filtering_resampling.html\" target=\"_blank\"><u>here</u></a>. \n",
    "You may want to read the next chapter to learn about filtering before looking at this tutorial though.\n",
    "\"\"\"\n",
    "\n",
    "code_panel = VBox([HTML(text), code, HTML(text2)])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4, code_panel], [\"1\",\"2\", \"3\", \"Check real data\", \"Code\"])\n",
    "display(chapter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33c9c6-ec63-496c-a293-6eb496af7d6b",
   "metadata": {},
   "source": [
    "# (8) Applying filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e423c22a-5b3f-4693-a177-b51fa205e852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d765be905b34805ac01621c00e9d40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value=\"\\nWe just learned that there are <i>typical</i> frequeny bandwidths for EEG data and…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "We just learned that there are <i>typical</i> frequeny bandwidths for EEG data and with the spectrum plot we saw a visualization that allows us to analyze these frequencies. \n",
    "We will use this knowledge to apply <b>filters</b> to our data now. \n",
    "The idea behind applying filters is quite straigth forward: We can assume that very high or very low frequencies don't come from the brain. \n",
    "Hence they must have a different source which is noise for us. Filters allow us to remove certain frequencies from the data. \n",
    "\"\"\"\n",
    "\n",
    "panel_1 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "The most-used filters are <b>highpass</b> filters and <b>lowpass</b> filters. \n",
    "A highpass filter removes all frequencies that are <i>lower</i> than a given threshold (in other words, it allows all <i>high</i> frequencies to <i>pass</i>).\n",
    "A lowpass filter removes all frequencies that are <i>higher</i> than a given threshold (in other words, it allows all <i>low</i> frequencies to <i>pass</i>).\n",
    "There is not <i>the</i> one and only value for the thresholds, but let us assume, that all frequencies below 0.1Hz and above 100Hz can not come from brain activity. \n",
    "In this case, we would apply a lowpass filter of 100Hz and a highpass filter of 0.1Hz. \n",
    "\"\"\"\n",
    "\n",
    "panel_2 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "Do you see the small peak in power around 50Hz in the following picture?\n",
    "\"\"\"\n",
    "im_power_noise = Image(value=open(\"resources/power_noise.png\", \"rb\").read(),width=width,height=height)\n",
    "text2 = \"\"\"\n",
    "In europe, the electric current coming from the power socket has a frequency of 50Hz and causes artifacts in our data. \n",
    "In the USA, the electric current has a frequency of 60Hz, so we would expect this peak to be at a slightly different position.\n",
    "<br>\n",
    "It is not a good idea to just remove the frequency of 50Hz, because that could also eliminate <i>real</i> eeg data. \n",
    "Luckily, there are different algorithms that allow to reduce the power line noise without distorting the data too much. \n",
    "Zipline (and its sucessor ZiplinePlus) are a good choice for removing the power noise. \n",
    "\"\"\"\n",
    "panel_3 = VBox([HTML(text), im_power_noise, HTML(text2)])\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Try applying different filters yourself! \n",
    "You can chose the thresholds for the high- and lowpass filters with the slider. \n",
    "Note that you can also input numeric vaues next to the slider!\n",
    "Be aware that applying zipline is computanionally expensive and might take a few minutes!\n",
    "\"\"\"\n",
    "def apply_zipline():\n",
    "    from meegkit import dss\n",
    "    from meegkit.utils import create_line_data, unfold\n",
    "    from mne.io import RawArray\n",
    "    # note the need to transpose the data before applying dss_line and transposing again before aplpying RawArray as described here: \n",
    "    # https://mne.discourse.group/t/clean-line-noise-zapline-method-function-for-mne-using-meegkit-toolbox/7407/13\n",
    "    data = raw.copy().get_data()\n",
    "    sfreq = raw.info['sfreq']\n",
    "    out, _ = dss.dss_line_iter(data.T, 50, sfreq) # 50 Hz for Europe\n",
    "    raw_zipline = RawArray(out.T, raw.info)\n",
    "\n",
    "output_filters = Output()\n",
    "def apply_filters(b):\n",
    "    low, high = filter_thresholds_slider.value\n",
    "    x = raw.copy().load_data().filter(l_freq=low, h_freq=high)\n",
    "\n",
    "    if zipline_checkbox.value is True:\n",
    "        x = apply_zipline()\n",
    "    psd = x.compute_psd(fmax=high*1.2)\n",
    "    psd_plot = psd.plot(show=False)\n",
    "    with output_filters:\n",
    "        clear_output(wait=True)\n",
    "        display(psd_plot)\n",
    "        \n",
    "filter_thresholds_slider = FloatRangeSlider(value=[0.1,100], min=0, max=150, readout=True)\n",
    "apply_filter_button = Button(description=\"apply filters\")\n",
    "apply_filter_button.on_click(apply_filters)\n",
    "zipline_checkbox = Checkbox(value=False, description=\"Apply Zipline\")\n",
    "\n",
    "panel_4 = VBox([HTML(text), HBox([filter_thresholds_slider, zipline_checkbox,apply_filter_button]), output_filters])\n",
    "\n",
    "text = \"\"\"\n",
    "In MNE, filters can be applied as follows, where low and high are the thresholds for the lowpass and the highpass filter:\n",
    "\"\"\"\n",
    "\n",
    "code = code_block(\"\"\"\n",
    "    filtered_data = raw.filter(l_freq=low, h_freq=high)\n",
    "\"\"\", links = {\"filter\":\"https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.filter\"})\n",
    "code_panel = VBox([HTML(text), code])\n",
    "\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4, code_panel], [\"1\",\"2\",\"3\", \"Try it yourself\", \"Code\"])\n",
    "display(chapter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fbb05e-66e4-463e-ba86-47ab81715905",
   "metadata": {},
   "source": [
    "# (9) Setting the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a8fe7e6a-bd76-4653-86d6-b8d5e4a1eea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e8661e57c347d1bcf81f4f5393725a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\nWe already learned, that an EEG measures brain activity in terms of electric poten…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "text = \"\"\"\n",
    "We already learned, that an EEG measures brain activity in terms of electric potential and hence power (measured in micro-Volt) over time. \n",
    "However, the pure measure of electric voltage is hard to interpret. Is 5 micro-Volt a lot? \n",
    "To analyze the data, it is recommended to compare it with a reference, as this allows to tell if a certain activity is stronger than this reference or less strong. \n",
    "Such a reference also allows to get rid of noise that affects all electrodes at the same time. \n",
    "<br>\n",
    "Different references are possible. The most frequently used are:\n",
    "<ul>\n",
    "<li>Having one or more reference electrodes near the head but far away from the brain, e.g. on the earlobe.</li>\n",
    "<li>Using the average of all electrodes as reference. After doing this, the data naturally indicates if an electric activity of a given electrode is above or below the average activity.</li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "\n",
    "panel_1 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "Here you can see how the spectrum of your data changes, if you use different references. \n",
    "What happens if you use a <i>normal</i> eeg channel as reference? How does this differ from using the average or an electrode that is placed outside of the brain region?\n",
    "\"\"\"\n",
    "import ipyspin\n",
    "output_reference = Output()\n",
    "reference_dropdown = Dropdown(options = [\"average\"] + raw.ch_names)\n",
    "def display_reference_change(b):\n",
    "    with output_reference:\n",
    "        x = raw.copy().load_data()\n",
    "        ref = reference_dropdown.value\n",
    "        ref = [ref] if ref != \"average\" else \"average\"\n",
    "        with_new_reference, _ = mne.set_eeg_reference(x,ref, ch_type=\"eeg\")\n",
    "\n",
    "        psd = with_new_reference.compute_psd(fmax=120)\n",
    "        psd_plot = psd.plot(show=False)\n",
    "        clear_output(wait=True)\n",
    "        display(psd_plot)\n",
    "b = Button(description=\"Change reference\")\n",
    "b.on_click(display_reference_change)\n",
    "display_reference_change(None)\n",
    "panel_2 = VBox([HBox([reference_dropdown, b]), output_reference])\n",
    "\n",
    "text = \"\"\"\n",
    "Setting the reference is possible with the following MNE function. \n",
    "You can either set it to the average, or you can give a list of channel names to use as a reference. \n",
    "The list may also contain just a singel channel. \n",
    "\"\"\"\n",
    "\n",
    "code = code_block(\"\"\"\n",
    "    raw = raw.set_eeg_reference(\"average\")\n",
    "\"\"\", links={\"set_eeg_reference\":\"https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.set_eeg_reference\"})\n",
    "text2 = \"\"\"\n",
    "<a href=\"https://mne.tools/stable/auto_tutorials/preprocessing/55_setting_eeg_reference.html#sphx-glr-auto-tutorials-preprocessing-55-setting-eeg-reference-py\" target=\"_blank\"><u>This tutorial</u></a> explains different ways to set a reference in more detail.\n",
    "\"\"\"\n",
    "\n",
    "code_panel = VBox([HTML(text), code, HTML(text2)])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, code_panel], [\"1\", \"Try it yourself\", \"Code\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa67f4-840d-46c6-8690-386a7c4c3cca",
   "metadata": {},
   "source": [
    "# (10) Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6cfb8e8d-153d-4705-8d03-a8637bea7535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332aa745fa2b4f4abcf8ed67a450df04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value=\"\\nSo far, we have always asumed that eeg data is a continous signal w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "So far, we have always asumed that eeg data is a continous signal with one timestep after the other. \n",
    "If you put an eeg cap on someones had and measure the brain activity for half an hour, this is a continuos signal, indeed. \n",
    "However, most of the time researchers are not interested in the brain activity over a long period of doing nothing. \n",
    "Instead, they want to measure the brain activity <i>in response to a stimulus or an activity</i> and conduct an experiment for that. \n",
    "<br>\n",
    "For example, you might be interested in the brain activity of participants if they see a red light and want to compare it to a green light. \n",
    "In this case, you would show them a red and a green light multiple times and measure the brain activity with an EEG. \n",
    "The crucial point is, that the EEG data needs to be aligned with the presentations of the stimuli. \n",
    "You need to know, when exactly the stimulus where shown and which part of the eeg data belongs to these moments in time. \n",
    "If you just have a continous signal like this, you don't know what happened during the collection of this activity:\n",
    "\"\"\"\n",
    "epoch_artificial_example = Image(value=open(\"resources/epoch_artificial_example.png\", \"rb\").read(),width=300,height=200)\n",
    "text2 = \"\"\"\n",
    "But if I tell you when exactly the red light had been shown (indicated by the grey areas in the following), you see that there is a connection between the stimulus and the brain activity.\n",
    "\"\"\"\n",
    "epoch_artificial_example_with_annotations = Image(value=open(\"resources/epoch_artificial_example_with_annotations.png\", \"rb\").read(),width=300,height=200)\n",
    "panel_1 = VBox([HTML(text), epoch_artificial_example, HTML(text2), epoch_artificial_example_with_annotations])\n",
    "\n",
    "text = \"\"\"\n",
    "Points in time that indicate something important happening in the experiment are called <b>events</b>. \n",
    "Typically that would be the presentation of a stimulus or the participant's response. \n",
    "For most research questions, you are interested in the brain activity <i>during the events</i>. \n",
    "For example, you might want to know how the brain activity changes if a red light is shown or how it looks like before the partipant presses a button. \n",
    "<br>\n",
    "When we take all parts of the data that belong to the same event, we call that <b>epochs</b>. \n",
    "\"\"\"\n",
    "panel_2 = HTML(text)\n",
    "\n",
    "epoch_plot_0 = Image(value=open(\"resources/epoch_plot_0.png\", \"rb\").read(),width=width,height=height)\n",
    "text = \"\"\"\n",
    "Epochs are often visualized like this. As this plot is not trivial, let us take a moment to understand what it shows. \n",
    "In the lower part, you see a figure with the time on the x-axis and the voltage on the y-axis. \n",
    "This is the average signal of all the single epochs we selected. \n",
    "Do you see, that the x-axis doesn't start at zero? Instead, the zero is marked with a dashed line. \n",
    "That is the point in time where the event starts. \n",
    "\"\"\"\n",
    "panel_3 = VBox([HTML(text), epoch_plot_0])\n",
    "\n",
    "text = \"\"\"\n",
    "However, it can be interesting to see how the signal behaved previously.\n",
    "If you see the signal rising even before the event started, you might wonder if there is any influence on the signal, that is not coming from the event itself. \n",
    "In the case shown below, the signal is quite linear and varying only very little though. \n",
    "This indicates, that the brain is in some <i>rest</i> state without much activity before the event happens. \n",
    "\"\"\"\n",
    "epoch_plot_1 = Image(value=open(\"resources/epoch_plot_1.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_4 = VBox([HTML(text), epoch_plot_1])\n",
    "\n",
    "text = \"\"\"\n",
    "A little later we see the voltage rising with a peak at roughly x=0.35.\n",
    "This could be a response to the stimulus shown in this event and this it what researchers search for most of the time.\n",
    "\"\"\"\n",
    "epoch_plot_2 = Image(value=open(\"resources/epoch_plot_2.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_5 = VBox([HTML(text), epoch_plot_2])\n",
    "\n",
    "text = \"\"\"\n",
    "Let us now take a look at the plot on the top. The x-axis is the same as previously and indicates the time.\n",
    "However, on the y-axis we see the different epochs (14, in this case). \n",
    "Each colored bar, ranging from x=-0.2 to x=0.5 is one epoch. \n",
    "In the following image I highlighted the epoch number 12. \n",
    "The voltage of the signal is indicated by the color. \n",
    "As can be seen on the right, dark red is a high voltage, lighter colors are lower voltage. \n",
    "With such a plot, you can easily see in how far the epochs are equal to each other. \n",
    "Epoch number 12 has a peak at roughly x=0.25, which is quite similar to some other epochs, that have this peak a little later. \n",
    "However, there are also epochs that don't show this peak (e.g. epoch 0 or epoch 4). \n",
    "\"\"\"\n",
    "epoch_plot_3 = Image(value=open(\"resources/epoch_plot_3.png\", \"rb\").read(),width=width,height=height)\n",
    "text_2 = \"\"\"\n",
    "A plot like this can help us assessing whether a mean signal is representing the <i>typical</i> epoch behavior well. \n",
    "In the following there is an example that should make us suspicious. Do you see, that there is one epoch that stands out? \n",
    "\"\"\"\n",
    "outstanding_epoch = Image(value=open(\"resources/outstanding_epoch.png\", \"rb\").read(),width=width,height=height)\n",
    "panel_6 = VBox([HTML(text), epoch_plot_3, HTML(text_2), outstanding_epoch])\n",
    "\n",
    "events, event_dict = events_from_annotations(raw)\n",
    "figures = []\n",
    "e = Epochs(raw, events, event_dict)\n",
    "for key,value in event_dict.items():\n",
    "    figures.append(e[key].plot_image(show=False)[0])\n",
    "text = \"\"\"\n",
    "Here you can select the different epochs of your data. Do you notice something interesting? Are there any epochs that stand out?\n",
    "\"\"\"\n",
    "panel_7 = VBox([HTML(text), get_figure_selector(figures)])\n",
    "plt.clf()\n",
    "\n",
    "text = \"\"\"\n",
    "MNE has an <a href=\"https://mne.tools/stable/generated/mne.Epochs.html\" target=\"_blank\"><u>Epoch class</u></a> to handle events. \n",
    "<br>\n",
    "Visualizations of epochs can be obtained with the plot_image function:\n",
    "\"\"\"\n",
    "code = code_block(\"\"\"\n",
    "epoch.plot_image()\n",
    "\"\"\", links={\"plot_image\":\"https://mne.tools/stable/generated/mne.Epochs.html#mne.Epochs.plot_image\"})\n",
    "\n",
    "code_panel = VBox([HTML(text), code])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4, panel_5, panel_6, panel_7, code_panel], [\"1\",\"2\", \"3\", \"4\",\"5\", \"6\", \"Try it yourself!\", \"Code\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7775a-0048-4182-bc1a-9c67bda76fbc",
   "metadata": {},
   "source": [
    "# (11) Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "895d6403-5d2e-49a8-af71-5ae9e0608304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d7cd118d349628c5abd7e7325a325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\nWe have to talk about noise again. \\n<br>\\nSo far we have already seen some method…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "We have to talk about noise again. \n",
    "<br>\n",
    "So far we have already seen some methods to remove noise: We can apply highpass and lowpass filters and we can remove the power line noise with Zipline. \n",
    "However, there are different kinds of noise that is very important to understand that are <b>artifacts</b>. \n",
    "<br>\n",
    "Artifacts are patterns that typically occur during some activity that is not brain activity itself. The most common artifacts include: \n",
    "<ul>\n",
    "<li>Eye blinks</li>\n",
    "<li>Horizontal Eye movements</li>\n",
    "<li>Heartbeat</li>\n",
    "<li>Muscle movements</li>\n",
    "<li>Broken sensors</li>\n",
    "</ul>\n",
    "\n",
    "Each of these can produce electric activity that changes the EEG measurement and hence distorts the signal coming from the brain. \n",
    "It is of utmost importance to filter these artifacts before analyzing EEG data, because the patterns coming from these artifacts are so big, that they overlay the actual brain activity patterns.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "panel_1 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "Fortunately, the aforementioned artifacts don't produce noise that is completely random, but show patterns that can be understood and detected. \n",
    "<br>\n",
    "Let us think of a horizontal eye movement as an example. \n",
    "The frontal part of your eye (the cornea) is charged positively, while the back part of your eye is charged negatively. \n",
    "If you look to the right, without moving the head, the positive potential is closer to the electrodes on the right side, while the negative parts of the eyes are closer to the electrodes on the left. \n",
    "As a consequence, a channel like F8 would record higher voltages and F7 would record lower voltages (remember how the channel name indicates the position on the skull?).\n",
    "In the eeg, you observe a pattern that looks like this:\n",
    "\"\"\"\n",
    "im_eye_movement_example = Image(value=open(\"resources/eye_movement_example.png\", \"rb\").read(),width=300,height=200)\n",
    "text2 = f\"\"\"\n",
    "{get_image_caption(\"https://www.learningeeg.com/artifacts\")}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "panel_2 = VBox([HTML(text), im_eye_movement_example, HTML(text2)])\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "For other kinds of artifacts there are other typically patterns that can be observed in the eeg data. \n",
    "\n",
    "People who become experts in working with EEG data learn to identify these patterns by sight. \n",
    "If you want to learn more about them, an overview of the typical patterns of various artifacts can be found <a href=\"https://www.learningeeg.com/artifacts\" target=\"_blank\"><u>on this site</u></a>.\n",
    "\"\"\"\n",
    "\n",
    "panel_3 = HTML(text)\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3], [\"1\",\"2\", \"3\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb8d15-8631-4172-abc6-4d90368ffbfc",
   "metadata": {},
   "source": [
    "# (12) Performing ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "03fd6df1-9c1a-42f1-9498-b87777cb02bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab4e13770fd41dfa3c6633e375de9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\nWe just learned, that there are artifacts that reflect activity that is not brain …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://vocal.com/blind-signal-separation/independent-component-analysis/\n",
    "\n",
    "text = \"\"\"\n",
    "We just learned, that there are artifacts that reflect activity that is not brain activity and hence disturbing our signal. \n",
    "We are able to identify these by looking at the EEG data, but what should we do, when we identify them? \n",
    "Do we need to throw away all points in time, where an eye blink, an eye movement or a heartbeat occured? \n",
    "<br>\n",
    "Not exactly. We will now introduce a method how to remove the influences of the artifacts without destroying the entire signal. \n",
    "This method is called <b>Independent Component Analysis (ICA)</b> and allows to represent the data in a different way that allows us to identify the sources of the artifacts more explicitly.\n",
    "\"\"\"\n",
    "panel_1 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "Before we dive into the details on how ICA works, we first need to understand what problem ICA solves at all. \n",
    "Let us start with an illustrative example. Say you have two people speaking at the same time. \n",
    "You have two microphones, one in front of the one person and one in fron of the other. \n",
    "However, as both persons are in the same room, the microphon in front of Person 1 also captures some sound of Person 2 speaking and vice versa. \n",
    "This scenario would look like this:\n",
    "\"\"\"\n",
    "im_ica_example_0 = Image(value=open(\"resources/ica_example_0.jpg\", \"rb\").read(),width=200,height=300)\n",
    "text2 = f\"\"\"\n",
    "{get_image_caption(\"https://vocal.com/blind-signal-separation/independent-component-analysis/\")}\n",
    "<br>\n",
    "You might have hoped that microphon 1 records the sound of Person 1 (and nothing else) and microphon 2 does so for Person 2, but that is not how it works. \n",
    "You do have two sound recordings now, but both include sound of both persons speaking. \n",
    "However, they do differ in the signal strength. Microphon 1 recorded <b>more from person 1</b> and microphon 2 recorded <b>more from person 2</b>. \n",
    "Is there any way now to separate the recordings to have the pure sound of person 1 as one signal and the pure sound of person 2 as the other signal?\n",
    "<br>\n",
    "This is exactly the problem that ICA solves. \n",
    "\"\"\"\n",
    "text3 = f\"\"\"\n",
    "{get_image_caption(\"https://vocal.com/blind-signal-separation/independent-component-analysis/\")}\n",
    "<br>\n",
    "Do you see the relation to EEG data? \n",
    "In an eeg you have many sensors (the microphones) and you have many sources of signals (the brain, the eye movements, the heartbeat...). \n",
    "The sensors record the activity of the various sources with different amounts. \n",
    "The sensors close to the eyes might be influenced more strongly by the eye movements than the sensors at the back of the head. \n",
    "You now want to separate the different sources. In the best case, you would have one signal that is <i>exactly</i> the activity of the eye movements, \n",
    "another signal that is <i>exactly</i> the heartbeat, another one that is <i>exactly</i> the brain activity and so on. \n",
    "\"\"\"\n",
    "im_ica_example_1 = Image(value=open(\"resources/ica_example_1.jpg\", \"rb\").read(),width=400,height=600)\n",
    "panel_2 = VBox([HTML(text), im_ica_example_0, HTML(text2), im_ica_example_1, HTML(text3)])\n",
    "\n",
    "text = \"\"\"\n",
    "We won't go into the details of the mathematical functions applied by ICA here. \n",
    "For us it is important to understand, that ICA takes the channels from the eeg and creates new <b>components</b> from it. \n",
    "You can think of a component as a <i>virtual channel</i>. \n",
    "Each of these channels has a signal over time and if you plot it, it looks like the raw eeg data you are already familiar with:\n",
    "\"\"\"\n",
    "im_ica_example_2 = Image(value=open(\"resources/ica_example_2.png\", \"rb\").read(),width=400,height=600)\n",
    "text2 = \"\"\"\n",
    "However, the ICA components are not <i>real</i> channels but <i>virutal</i> ones, because there is no exact sensor that recorded the signal we see in ICA0001 in the previous plot.\n",
    "Instead, the ICA components are weighted combinations of the original channels. \n",
    "Say we have 3 original channels. One ICA component could be 1.5*channel_0  + 0.1*channel_1 - 2.5*channel_2, for example.\n",
    "These weights tell us, how strong the influence of the original channels are on the components. \n",
    "In this example, channel_1 only contributes very little to the component (because of the small weights). \n",
    "Strong negative weights, however, also indicate a <i>strong</i> influence. Channel_2 has the strongest influence in this example. \n",
    "Whenever channel_2 changes, the component changes in the opposite direction by a factor of 2.5. \n",
    "\"\"\"\n",
    "answer = get_answer_to_reveal(\"Say you have a component with the weights 0.1*channel_0 -0.1*channel_1 + 1*channel_2. What does this tell you about the relation between the original channels and this component? Does this give you any information about the sensor of channel_2?\",\n",
    "                             \"The components is almost identical to the original channel_2. That could mean, that this sensor indeed captured a signal that no other sensor captured.\")\n",
    "panel_3 = VBox([HTML(text), im_ica_example_2, HTML(text2), answer])\n",
    "\n",
    "code = code_block(\"ica.get_components()\", links={\"get_components\":\"https://mne.tools/stable/generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.get_components\"})\n",
    "text = f\"\"\"\n",
    "We will now deepen our understanding of the weights with a real example.\n",
    "<br>\n",
    "After having performed the ICA, we can get a component matrix.\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "In our example it has the shape (62,13), because we have 62 channels and ICA identified 13 components. \n",
    "This 62x13 matrix contains the weights that connect the ICA components with the channels.\n",
    "<br>\n",
    "If we take a look at a single row of the matrix (say, row number 2), we get the weights that tell us how the different channels contribute to this component.\n",
    "These weights are visualized in the following. Note that there are 62 weights, one for each channel. \n",
    "\"\"\"\n",
    "im_ica_weights = Image(value=open(\"resources/ica_component_weights.png\", \"rb\").read(),width=400,height=600)\n",
    "answer = get_answer_to_reveal(\"Do you see which channels have strong influence on this component and which have a less strong influence?\",\n",
    "                             \"\"\"Channel 2 and 33 have very strong influence, because they have the highest weights. \n",
    "                             Channel 3 and 56 also have strong influence, because they have large negative weights. \n",
    "                             Channel 4 has almost no influence on this component, as the weight is so small.\n",
    "                             \"\"\")\n",
    "panel_4 = VBox([HTML(text), code, HTML(text2), im_ica_weights, answer])\n",
    "\n",
    "text = \"\"\"\n",
    "The channels that had the highest influence in the previous panel were FP2 and AF8. \n",
    "The channels with the highest negative influence were F3 and FT7. \n",
    "Do you remember where these channels are located on the skull?\n",
    "<br>\n",
    "We can plot the weights of the different channels together with their location and get a visualization like this:\n",
    "\"\"\"\n",
    "im_ica_topomap = Image(value=open(\"resources/ica_component_example_2.png\", \"rb\").read(),width=400,height=600)\n",
    "text2 = \"\"\"\n",
    "You see high weights displayed in red and high negative weights displayed in blue. \n",
    "Do you see that there are a few channels that have strong influence while many others have only very weak influence on the component?\n",
    "Also note the pattern of the channels with strong influence.\n",
    "They are located frontal and on both sides of the skull. \n",
    "This indicates that the source of the signal that creates this ICA component might come from a frontal region. \n",
    "\"\"\"\n",
    "panel_5 = VBox([HTML(text), im_ica_topomap, HTML(text2)])\n",
    "\n",
    "text = \"\"\"\n",
    "We have seen, that ICA created components that are weighted combinations of the original channels.\n",
    "These components are created in such a way, that they are <i>maximally different from each other</i>, which reflects the idea of capturing different signals in the components (think of the different persons we recorded with different microhpones).\n",
    "We haven't talked about the number of components yet, which we want to do in the following.\n",
    "<br>\n",
    "If you have n channels, you can create n ICA components. \n",
    "However, you can also create less. If you have many sensors on your eeg cap, some of them might record very similar data. \n",
    "By lowering the number of ICA components, you can reduce the amount of data without loosing much information. \n",
    "<br>\n",
    "There are two ways of determining the number of components. \n",
    "<br>\n",
    "<b>First</b>, you can specify a number and ICA will produce exactly that many components.\n",
    "<br>\n",
    "<b>Second</b>, you can specify an amount of explained variance. \n",
    "For example, if you set this amount to 0.95, you expect ICA to create as many components as needed to keep 95% of the information in the data (and loose only 5%). \n",
    "By that you can reduce the amount of data by quite a lot, if you are willing to afford a small loss of information. \n",
    "\"\"\"\n",
    "panel_6 = HTML(text)\n",
    "\n",
    "text = \"\"\"\n",
    "MNE can compute an ICA with the following code, where n_components can either be an integer number of ica components or a float between 0 and 1 that indicates how much variance should be explained by the components.\n",
    "\"\"\"\n",
    "\n",
    "code1 = code_block(\"\"\"from mne.preprocessing import ICA\n",
    "    ica = ICA( n_components=0.95, max_iter=\"auto\")\n",
    "    ica.fit(raw)\"\"\", links={\"ICA\":\"https://mne.tools/stable/generated/mne.preprocessing.ICA.html\", \"fit\":\"https://mne.tools/stable/generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.fit\"})\n",
    "\n",
    "text2 = \"\"\"\n",
    "The weights that connect the ica components with the orginal channels are given in the get_components method:\n",
    "\"\"\"\n",
    "text3 = \"\"\"The plot showing the weights' distribution over the skull can be obtained with the following function, where idx is the index of the component to plot.\n",
    "\"\"\"\n",
    "code3 = code_block(\"\"\"ica.plot_components(idx)\"\"\", links={\"plot_components\": \"https://mne.tools/stable/generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.plot_components\"})\n",
    "code_panel = VBox([HTML(text), code1, HTML(text2), code, HTML(text3), code3 ])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4, panel_5, panel_6, code_panel], [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"Code\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed342b21-bd09-4f1e-8256-014e37116565",
   "metadata": {},
   "source": [
    "# (13) Assessing ICA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e6c5eca7-3b43-476b-92bd-87cf4f16f9d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>infomax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit parameters</th>\n",
       "        <td>extended=True<br />max_iter=500<br /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>500 iterations on raw data (153600 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | raw data decomposition, method: infomax (fit in 500 iterations on 153600 samples), 15 ICA components (62 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mne.preprocessing import ICA\n",
    "raw_for_ica = raw.copy().load_data()\n",
    "raw_for_ica,_ = mne.set_eeg_reference(raw_for_ica,\"average\", ch_type=\"eeg\")\n",
    "raw_for_ica.filter(l_freq=1, h_freq=100)\n",
    "# We use the extended infomax algorithm here (instead of standard fastica), as this is expected by ICLabel. \n",
    "ica = ICA( n_components=0.95, max_iter=\"auto\", random_state=97, method=\"infomax\", fit_params=dict(extended=True), verbose=True)\n",
    "ica.fit(raw_for_ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05ace5d4-3695-4ced-95f6-65c8d511472f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04be6c8c834c41f5ad21f6ff438901ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value=\"\\nWe now know how to compute ICA components. \\nThe next step is to fi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "text = \"\"\"\n",
    "We now know how to compute ICA components. \n",
    "The next step is to find out, which kinds of signals these components represent. \n",
    "Remember, that we still want to filter artifacts like eye blinks or heartbeats. \n",
    "Our hope is, that some of the components capture these artifacts while other components capture the actual brain activity.\n",
    "<br>\n",
    "There are different critera to consider when identifying ICA components as artifacts, that are\n",
    "<ul>\n",
    "<li> Patterns in the component's eeg data</li>\n",
    "<li> The distribution of the channels involved in the component</li>\n",
    "<li> The shape of the power spectrum</li>\n",
    "<li> Additional features derived from the component's data</li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "panel_1 = VBox([HTML(text)])\n",
    "\n",
    "text = \"\"\"\n",
    "We have already seen some <b>patterns</b> of artifacts before in the example of the horizontal eye-movements.\n",
    "The following visualization gives an example of a typicall eye-blink pattern (first row) and horizontal eye-movement pattern (second row).\n",
    "\"\"\"\n",
    "\n",
    "im_ica_curve_patterns = Image(value=open(\"resources/ica_curves_artifacts.png\", \"rb\").read(),width=400,height=600)\n",
    "\n",
    "\n",
    "panel_2 = VBox([HTML(text), im_ica_curve_patterns])\n",
    "\n",
    "text = \"\"\"\n",
    "The <b>sensors involved</b> in a component (i.e., those sensors with high weights) can be indicative for the kinds of artifacts a component captures. \n",
    "We already saw that for eye movements, the frontal sensors are included with a typical pattern of negative activity on the one and positive activity on the other side. \n",
    "For other artifacts, typical distributions of the component weights can be specified:\n",
    "\"\"\"\n",
    "im_ica_location_eye_movement = Image(value=open(\"resources/ica_location_eye_movement.png\", \"rb\").read(),width=400,height=600)\n",
    "im_ica_location_blink = Image(value=open(\"resources/ica_location_blink.png\", \"rb\").read(),width=400,height=600)\n",
    "im_ica_location_muscle = Image(value=open(\"resources/ica_location_muscle.png\", \"rb\").read(),width=400,height=600)\n",
    "\n",
    "text2 = \"\"\"\n",
    "The first image shows the typicall <b>eye-movement</b> distribution. \n",
    "The second shows, that for <b>eye blinks</b> you expect the frontal regions to show negative activity for both sides of the head.\n",
    "The third image shows a typical distribution for <b>muscle activation</b>, that comes from the muscles behind the ears. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "panel_3 = VBox([HTML(text), HBox([im_ica_location_blink, im_ica_location_eye_movement, im_ica_location_muscle]), HTML(text2)])\n",
    "\n",
    "text = \"\"\"\n",
    "The <b>shape of the power spectrum</b> can <i>sometimes</i> be used as a hint to identify an artifact. \n",
    "For example, for activity coming from a muscle component, we expect quite high power at high frequencies, as can be seen here:\n",
    "\"\"\"\n",
    "im_power_spectrum_muscle_artifact = Image(value=open(\"resources/power_spectrum_muscle_artifact.png\", \"rb\").read(),width=400,height=600)\n",
    "text2 = f\"\"\"\n",
    "{get_image_caption(\"https://www.sciencedirect.com/science/article/pii/S0165027015000928?fr=RR-2&ref=pdf_download&rr=8bd3c4788fbc9f36\")}\n",
    "<br>\n",
    "If you compare this with the <i>typical</i> power spectrum of neural activity (next figure), you see it's higher power at high frequencies.\n",
    "\"\"\"\n",
    "im_power_spectrum_neural = Image(value=open(\"resources/power_spectrum_neural.png\", \"rb\").read(),width=400,height=600)\n",
    "text3 = f\"\"\"\n",
    "{get_image_caption(\"https://www.sciencedirect.com/science/article/pii/S0165027015000928?fr=RR-2&ref=pdf_download&rr=8bd3c4788fbc9f36\")}\n",
    "\"\"\"\n",
    "panel_4 = VBox([HTML(text), im_power_spectrum_muscle_artifact, HTML(text2), im_power_spectrum_neural, HTML(text3)])\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Some approaches to identify artifacts propose to compute <b>features</b> from the data. \n",
    "A feature can be anything that can be calculated from the data and that is expected to be (partially) indicative for an artifact. \n",
    "<br>\n",
    "For example, you can compute the <b>Spatial Average Difference (SAD)</b> that is the difference between the activation of the frontal sensors and the posterior sensors.\n",
    "If this difference is high, this speaks for an eye-blink pattern. \n",
    "<br>\n",
    "Another feature could be the maximum amplitude of a component. This feature could also be indicative of an eye-blink, but it could also be caused by a sensor malfunction.\n",
    "<br>\n",
    "You can easily come up with additional features. \n",
    "In addition to the features, you need some <i>rules</i> though, that tell which feature is indicative for which pattern. \n",
    "These rules can be explicit (like <i>high SAD is indicative for eye-blink</i>), but they can also be represented in a more complex manner. \n",
    "Features are what you typically use to train a machine learning model. \n",
    "This model decides on its own how to weight the features to become an indicator of an artifact, and applies this knowledge, when you use the trained model to classify your components.\n",
    "\"\"\"\n",
    "\n",
    "panel_5 = HTML(text)\n",
    "\n",
    "text = f\"\"\"\n",
    "in MNE, there is a function that allows to visualize multiple properties of an ICA component at the same time:\n",
    "{code_block(\"ica.plot_properties(raw_data, [component_number])\")}\n",
    "<br>\n",
    "\n",
    "\"\"\"\n",
    "im_ica_properties = Image(value=open(\"resources/ica_properties.png\", \"rb\").read(),width=400,height=600)\n",
    "text2 = \"\"\"\n",
    "You see multiple plots here, most of which you are already familiar with. \n",
    "<br>\n",
    "In the <b>bottom left</b> you see the power spectrum. \n",
    "<br>\n",
    "In the <b>top left</b> you see the weights of the sensors. \n",
    "<br>\n",
    "In the <b>top right</b> you see the activation in a plot you are already familiar with from the chapter about events. \n",
    "However, there is one important difference to the events we saw earlier. What we see here is a <b>segment plot</b>. \n",
    "The data has been split into different <b>segments</b>, i.e. timespans of the same duration, and these are visualized as rows. \n",
    "These segments are <i>not</i> aligned with the events though. \n",
    "That means, we don't know what happened in each of the segments. Maybe in one segment a stimulus has been shown and in another it has not. \n",
    "<br>\n",
    "The plot at the <b>bottom right</b> tells you, if some of the segments have been removed because they are very different from the others. Most of the time, this shouldn't concern you and doesn't help in identifying artifacts.\n",
    "\"\"\"\n",
    "answer = get_answer_to_reveal(\"If you take a look at the component shown in the previous plot, can you guess which artifact it shows?\", \"Eye-blink pattern\")\n",
    "text3 = \"\"\"\n",
    "The distribution of the sensors, with the frontal sensors being very active, is a hint for an eye-blink pattern. \n",
    "You may wonder though, why we don't see the typical spike patterns in the top left plot. \n",
    "Here is the answer: Eye-blinks are distributed randomly, i.e. they can occur at any point in time without any pattern or regularity. \n",
    "We just said that the segments shown here are timespans of equal length. \n",
    "If you take multiple of these timespans, an eye blink might occur at the very beginning, in the middle, at the end, somehwere in between or not at all.\n",
    "As a consequence, when you average the signal of multiple timespans, the eye-blink spikes are distributed equally over time and are averaged out. \n",
    "<br>\n",
    "This example should make you aware, that there is not <i>the one and only</i> visualization that allows you to find artifact patterns. \n",
    "If you look at the continous eeg instead of the segments, you find the eye-blink spikes easily. \n",
    "\"\"\"\n",
    "panel_6 = VBox([HTML(text), im_ica_properties, HTML(text2), answer, HTML(text3)])\n",
    "\n",
    "\n",
    "ica_property_figures = [ica.plot_properties(raw, [i], show=False, verbose=False)[0] for i in range(ica.n_components_)]\n",
    "\n",
    "left_panel = get_figure_selector(ica_property_figures, headline=\"Your Data\")\n",
    "\n",
    "\n",
    "def create_widgets(expected_properties, filenames):\n",
    "    t = VBox([Label(value=property) for property in expected_properties])\n",
    "    files = [open(filename, \"rb\") for filename in filenames]\n",
    "    images = [file.read() for file in files]\n",
    "    imgs = [Image(\n",
    "        value=image,\n",
    "        format='png',\n",
    "        width=300,\n",
    "        height=400,\n",
    "    ) for image in images]\n",
    "    \n",
    "    box = VBox([HBox(imgs), t])\n",
    "    return box\n",
    "\n",
    "\n",
    "# eye blink\n",
    "expected_properties = [\"Frontal topography\",\"Large amplitude\",\"Opposite polarity below the eys\",\"No peak at physiological frequencies\",\"High correlation with vertical EOGs\",\"High eye moevement related measures\"]\n",
    "filenames = [\"resources/blink_components_B_truncated.png\", \"resources/blink_components_C_truncated.png\"]\n",
    "eye_blink = create_widgets(expected_properties, filenames)\n",
    "# horizontal eye movement\n",
    "expected_properties = [\"Opposite sign bilateral frontal topography\", \"Step-like events\", \"Opposite polarity around the eyes\", \"No peak at physiological frequencies\", \"High correlation with vertical/horizontal EOGs\", \"High eye movement related measures\"]\n",
    "filenames = [\"resources/horizontal_eye_movement_E_truncated.png\", \"resources/horizontal_eye_movement_F_truncated.png\"]\n",
    "horizontal_eye_movement = create_widgets(expected_properties, filenames)\n",
    "# muscle components\n",
    "expected_properties = [\"Focal topography\", \"Steady noisy time courses dissipating / building up across trials\", \"Power at high frequencies\", \"High noise measures\"]\n",
    "filenames = [\"resources/muscle_components_B_truncated.png\", \"resources/muscle_components_C_truncated.png\"]\n",
    "muscle_components = create_widgets(expected_properties, filenames)\n",
    "# bad channel\n",
    "expected_properties = [\"Focal (one channel) topography\", \"Noisy time course\", \"High correlation with marked bad channel\", \"High spaital/intertrial noise measures\"]\n",
    "filenames = [\"resources/bad_channel_B_truncated.png\", \"resources/bad_channel_C_truncated.png\"]\n",
    "bad_channel = create_widgets(expected_properties, filenames)\n",
    "\n",
    "figures = [eye_blink,horizontal_eye_movement, muscle_components, bad_channel]\n",
    "right_panel = get_figure_selector(figures, headline=\"Reference\", button_labels={\"Eye blink\":0, \"Eye movement\":1, \"Muscle movement\":2, \"Bad channel\":3})\n",
    "text = \"\"\"\n",
    "In this panel you can take a look at ica components computed from real data, by selecting them with the buttons on the left side. \n",
    "In addition, you can see typical patterns of artifacts on the right side. \n",
    "Show both side by side and compare the actual data with the expecations. Do you see ica components, that show properties that indicate an artifact?\n",
    "\"\"\"\n",
    "left_panel.layout = Layout(flex='1 1 0', width='auto') \n",
    "right_panel.layout = Layout(flex='1 1 0', width='auto')  \n",
    "panel_n = VBox([HTML(text), HBox([left_panel, right_panel])])\n",
    "plt.clf()\n",
    "\n",
    "text = \"\"\"\n",
    "Visualizations of ICA components can be obtained as follows, where component_number is the index of the component to plot and ica is the already computed ica solution (see previous chapter).\n",
    "\"\"\"\n",
    "code = code_block(\"\"\"ica.plot_properties(raw, [component_number])\"\"\", links={\"plot_properties\":\"https://mne.tools/stable/generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.plot_properties\"})\n",
    "code_panel = VBox([HTML(text), code])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, panel_4, panel_5, panel_6, panel_n, code_panel], [\"1\",\"2\",\"3\",\"4\",\"5\", \"6\",\"Try it yourself!\", \"Code\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98374c84-8a7c-4ef6-933e-eb95b43e3299",
   "metadata": {},
   "source": [
    "# (14) ICLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fde38ca2-5113-4070-aa7d-bcdcf1b2c360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1744250cace8475b8d3cf8e950b92ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\nBy now you have gotten familiar with patterns of typical artifacts like eye-blink …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/mne-tools/mne-icalabel\n",
    "#https://mne.tools/mne-icalabel/stable/generated/examples/00_iclabel.html#sphx-glr-generated-examples-00-iclabel-py\n",
    "from mne_icalabel import label_components\n",
    "ic_labels = label_components(raw_for_ica, ica, method=\"iclabel\")\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "By now you have gotten familiar with patterns of typical artifacts like eye-blink or eye-movements. \n",
    "This knowledge can help you filter out these artifacts from the eeg data. \n",
    "However, inspecting the ICA components is high manual effort, especially when you have multiple dataset from various participants. \n",
    "<br>\n",
    "Fortunately, many approaches exist, that allow to support or (partially) automate the process of finding and removing ICA components that represent artifacts. \n",
    "The most widely used approach is called <b>ICLaleb</b>, which is a neural net that was trained on thousands of examples of ICA components to label them as one of the folllowing classes:\n",
    "<ul>\n",
    "<li>brain</li>\n",
    "<li>muscle artifact</li>\n",
    "<li>eye blink</li>\n",
    "<li>heart beat</li>\n",
    "<li>line noise</li>\n",
    "<li>channel noise</li>\n",
    "<li>other</li>\n",
    "</ul>\n",
    "Note that ICLabel does not distinguish between <i>eye blink</i> and <i>eye movements</i> and that <i>brain</i> is the label for the components that include no artifacts but <i>pure</i> brain activity.\n",
    "<br>\n",
    "Using ICLabel is very convenient: You just give it the data of your ICA components and it tells you the labels each with a <i>probability</i> that estimates how <i>certain</i> the model is about the prediction it made.\n",
    "\n",
    "\"\"\"\n",
    "panel_1 = HTML(text)\n",
    "code = code_block(\"\"\"\n",
    "from mne_icalabel import label_components\n",
    "ic_labels = label_components(raw, ica, method=\"iclabel\")\n",
    "\"\"\")\n",
    "text = f\"\"\"\n",
    "The predictions of ICLabel for the given ica components can easily be computed in python: \n",
    "{code}\n",
    "<br>\n",
    "This gives us the following predictions:\n",
    "<br>\n",
    "<ul>\n",
    "{\"\".join([f\"<li>Component: <b>{i}</b> Prediction: <b>{ic_labels['labels'][i]}</b> Probability: <b>{ic_labels['y_pred_proba'][i]}</b></li>\" for i in range(len(ic_labels[\"labels\"]))])}\n",
    "</ul>\n",
    "<br>\n",
    "However, be aware, that the model might also make mistakes and should not be trusted blindly. \n",
    "In the next panel, you can insepct the ica components again and see, if you agree with the model's predictions.\n",
    "\"\"\"\n",
    "panel_2 = HTML(text)\n",
    "\n",
    "prediction_labels = [f\"<h2>Prediction: {ic_labels['labels'][i]} (probability: {ic_labels['y_pred_proba'][i]})</h2>\" for i in range(len(ic_labels[\"labels\"]))]\n",
    "panel_3 = get_figure_selector(ica_property_figures, headline=\"Your Data\", ic_label_predictions = prediction_labels)\n",
    "\n",
    "text = \"\"\"\n",
    "ICLabel is not implemented in the MNE package itself, but there is an <a href=\"https://mne.tools/mne-icalabel/dev/index.html\" target=\"_blank\"><u>external python package called MNE-ICALabel</u></a> that provides ICLabel as a function that is perfectly compatible with MNE and can be used as follows, \n",
    "where raw and ica are the MNE objects of the raw data and the already computed ICA solution.\n",
    "\"\"\"\n",
    "code = code_block(\"\"\"from mne_icalabel import label_components\n",
    "    ic_labels = label_components(raw, ica, method=\"iclabel\")\"\"\", links={\"label_components\":\"https://mne.tools/mne-icalabel/dev/generated/api/mne_icalabel.label_components.html#mne_icalabel.label_components\"})\n",
    "\n",
    "code_panel = VBox([HTML(text), code])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2, panel_3, code_panel], [\"1\",\"2\",\"3\", \"Code\"])\n",
    "display(chapter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a730a5-8e56-4566-8702-dda92ce3651c",
   "metadata": {},
   "source": [
    "# (15) Removing ICA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "75d9843b-b8b2-4b23-b2bc-28f8774ecc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df909797949458b9a4b40aabb15b834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value=\"\\nNow that we have learned so much about labeling ICA components as r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Now that we have learned so much about labeling ICA components as representing artifacts, let's see what happens, when we remove them.\n",
    "In the following you can mark components to be excluded and see how that affects the eeg signal.\n",
    "You will see some plots that give you a first impression of the changes by showing properties of the original data in comparison to the data after excluding the components you want to exclude.\n",
    "Do you see how the signal changes? Which changes do you see in the power spectrum? \n",
    "Also take a look at the eeg signal in more detail (lower part of the panel). Is it less noisy than before?\n",
    "\"\"\"\n",
    "\n",
    "checkboxes = []\n",
    "for i in range(ica.n_components_):\n",
    "    check = Checkbox(description=f\"Exclude Component {i}\")\n",
    "    checkboxes.append(check)\n",
    "\n",
    "b = Button(description=\"Perform exclusion\")\n",
    "output_ica_exclusion = Output()\n",
    "def display_overlay(b):\n",
    "    exclusion_indices = []\n",
    "    for i,check in enumerate(checkboxes):\n",
    "        if check.value is True:\n",
    "            exclusion_indices.append(i)\n",
    "    fig = ica.plot_overlay(raw_for_ica, exclude=exclusion_indices, show=False, picks=\"eeg\")\n",
    "    with output_ica_exclusion:\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "    with raw_eeg_output:\n",
    "        raw_after_exclusion = ica.apply(raw, exclude=exclusion_indices)\n",
    "        raw_after_exclusion.plot(n_channels=4)\n",
    "b.on_click(display_overlay)\n",
    "\n",
    "raw_eeg_output = Output()\n",
    "\n",
    "panel = VBox([HTML(text), HBox([VBox(checkboxes+ [b]), output_ica_exclusion]), raw_eeg_output])\n",
    "\n",
    "text = \"\"\"\n",
    "The visualizations that show the difference between the data with vs. without excluding the components can be obtained as in the following code snippet, where exclusion_indices is a list of indices which components to exclude.\n",
    "\"\"\"\n",
    "code = code_block(\"\"\"ica.plot_overlay(raw_for_ica, exclude=exclusion_indices)\"\"\", links={\"plot_overlay\":\"https://mne.tools/stable/generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.plot_overlay\"})\n",
    "text2 = \"\"\"\n",
    "The permament exclusion of ica components from the data can be performed like this:\n",
    "\"\"\"\n",
    "code2 = code_block(\"\"\"raw_filtered = ica.apply(raw, exclude=[exclusion_indices])\"\"\", links={\"apply\":\"https://mne.tools/stable/generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.apply\"})\n",
    "\n",
    "code_panel = VBox([HTML(text), code, HTML(text2), code2])\n",
    "\n",
    "chapter = get_chapter([panel, code_panel], [\"1\", \"Code\"])\n",
    "display(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9aa9d8-4972-466b-9d4a-6f6ca8603a55",
   "metadata": {},
   "source": [
    "# Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "487e9c37-f625-40f6-912f-387d84ae1457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199f70fa71ac4a0caf935aa6b3b3337c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value=\"\\nLet's take a signal that is constructed as 3 * sin(x) + 0 * sin(2*x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter24.04-FFT-in-Python.html\n",
    "\n",
    "output_constructed_signal_example = Output()\n",
    "text = \"\"\"\n",
    "Let's take a signal that is constructed as 3 * sin(x) + 0 * sin(2*x) + 6 * sin(3*x) + 1 * sin(4*x). \n",
    "This signal looks as follows:\n",
    "\"\"\"\n",
    "\n",
    "x = np.arange(0,3, 0.01)\n",
    "x_pi = x*2*np.pi\n",
    "\n",
    "w1,w2,w3,w4 = 3,0, 6, 2\n",
    "y = w1 * np.sin(x_pi) + w2*np.sin(2*x_pi) + w3 * np.sin(3*x_pi) + w4 * np.sin(4*x_pi)\n",
    "with output_constructed_signal_example:\n",
    "    fig,axs = plt.subplots()\n",
    "    axs.plot(x,y)\n",
    "    display(fig)\n",
    "\n",
    "text2 = \"\"\"\n",
    "Applying the fourier transform now gives us data that can be visualized as follows:\n",
    "\"\"\"\n",
    "output_fourier_solution = Output()\n",
    "fourier = np.fft.fft(y)\n",
    "N  = len(y)\n",
    "n = np.arange(N)\n",
    "T = N / (1/0.01)\n",
    "freq = n / T\n",
    "with output_fourier_solution:\n",
    "    fig, axs = plt.subplots()\n",
    "    axs.stem(freq, abs(fourier), \"b\", markerfmt=\"\", basefmt=\"-b\")\n",
    "    axs.set_xlim(0, 10)\n",
    "    axs.set_xlabel(\"Frequency\")\n",
    "    axs.set_ylabel(\"Power\")\n",
    "    axs.set_yticks([])\n",
    "    display(fig)\n",
    "text3 = \"\"\"\n",
    "Do you see that there are three peaks at the positions x=1, x=3 and x=4?\n",
    "That is because our signal has been constructed by the frequency 1Hz, 3Hz and 4Hz. \n",
    "Also note that the peak at x=3 is highest and x=4 is the smallest. \n",
    "This indicates, that the 3Hz sine wave contributes highest power to the signal and the 4Hz sine wave contributes the least. \n",
    "\"\"\"\n",
    "panel_1 = VBox([HTML(text), output_constructed_signal_example, HTML(text2), output_fourier_solution, HTML(text3)])\n",
    "\n",
    "\n",
    "constructed_signal_output = Output()\n",
    "output_fourier_solution = Output()\n",
    "\n",
    "def show_constructed_signal(b):\n",
    "    x = np.arange(0,3, 0.01)\n",
    "    x_pi = x*2*np.pi\n",
    "\n",
    "    y = slider_w1.value * np.sin(x_pi) + slider_w2.value*np.sin(2*x_pi) + slider_w3.value * np.sin(3*x_pi) + slider_w4.value * np.sin(4*x_pi)\n",
    "    with constructed_signal_output:\n",
    "        fig,axs = plt.subplots()\n",
    "        axs.plot(x,y)\n",
    "        axs.set_ylim(-40, 40)\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "    fourier = np.fft.fft(y)\n",
    "    N  = len(y)\n",
    "    n = np.arange(N)\n",
    "    T = N / (1/0.01)\n",
    "    freq = n / T\n",
    "    with output_fourier_solution:\n",
    "        clear_output(wait=True)\n",
    "        fig, axs = plt.subplots()\n",
    "        axs.stem(freq, abs(fourier), \"b\", markerfmt=\"\", basefmt=\"-b\")\n",
    "        axs.set_xlim(0, 10)\n",
    "        axs.set_xlabel(\"Frequency\")\n",
    "        axs.set_ylabel(\"Power\")\n",
    "        axs.set_yticks([])\n",
    "        display(fig)\n",
    "\n",
    "slider_w1 = FloatSlider(min=0, max=10, value=1, description=\"1Hz freq\")\n",
    "slider_w2 = FloatSlider(min=0, max=10, value=1, description=\"2Hz freq\")\n",
    "slider_w3 = FloatSlider(min=0, max=10, value=1, description=\"3Hz freq\")\n",
    "slider_w4 = FloatSlider(min=0, max=10, value=1, description=\"4Hz freq\")\n",
    "\n",
    "b = Button(description=\"Construct waveform\")\n",
    "b.on_click(show_constructed_signal)\n",
    "\n",
    "\n",
    "\n",
    "panel_2 = HBox([VBox([HBox([slider_w1, slider_w2]), HBox([slider_w3, slider_w4]), b, constructed_signal_output]), output_fourier_solution])\n",
    "\n",
    "chapter = get_chapter([panel_1, panel_2], [\"1\", \"Try it yourself!\"])\n",
    "display(chapter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d1f69-f302-4093-a5f9-3e27a5c8e67c",
   "metadata": {},
   "source": [
    "# Future steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6367a9-e741-4cf6-8a4e-df662042f6e2",
   "metadata": {},
   "source": [
    "### Open questions\n",
    "* Why does the data change so much, if you take one *normal* sensor as reference?\n",
    "* Why are eye-blinks sometimes positive spikes, sometimes negative ones? Voltage is always relative\n",
    "* Does a segment plot make sense at all? In which cases would you expect a signal that is more than just a random average? Can be helpful to detect outliers\n",
    "\n",
    "### Potential additional chapters\n",
    "* More details about features and coresponding filter algorithms\n",
    "* Fourier Transformation and the concept of a signal as combination of pure sine waves (at the end; maybe mention in (4); assumptions of perfectly pure waves vs. burst in reality)\n",
    "* ERPs / ERSP (event related spectral perturbations)\n",
    "* Mentioning of additional measures (EOG, ECG, MEG...)\n",
    "\n",
    "### Ideas\n",
    "* Add references and links to text\n",
    "* Add references and functions for MNE code\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df91a5-482b-4c61-b5cf-3db7edff4144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (eeg)",
   "language": "python",
   "name": "eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
